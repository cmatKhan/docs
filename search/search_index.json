{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"High Throughput Computing Facility @ CGS_SB The Center for Genome Sciences and Systems Biology is the proud home of The High Throughput Computing Facility, a Washington University recharge center . The HTCF provides high-throughput computational resources for researchers within the CGS_SB. An HTCF description for grant writing purposes The CGSSB provides a computational cluster for high-throughput bioinformatics. The cluster consists of over 2750 processors and over 20TB of RAM. It supports the Center's Illumina sequencing platforms and real time sequencing analysis. Long term data storage is handled by our 10/40 GbE connected storage arrays. These arrays are currently over 2 petabytes in size. All user data is backed up and stored daily, weekly and monthly. A disaster recovery copy of select data is stored offsite. The HTCF also includes a 600TB high-speed distributed file system that is capable of throughput up to 19GB/s. This, coupled with its 100Gb network backbone, delivering 10Gb of bandwidth to cluster nodes, the HTCF can provide exceptionally high-speed transfer of large amounts of data.","title":"About"},{"location":"#high-throughput-computing-facility-cgs_sb","text":"The Center for Genome Sciences and Systems Biology is the proud home of The High Throughput Computing Facility, a Washington University recharge center . The HTCF provides high-throughput computational resources for researchers within the CGS_SB. An HTCF description for grant writing purposes The CGSSB provides a computational cluster for high-throughput bioinformatics. The cluster consists of over 2750 processors and over 20TB of RAM. It supports the Center's Illumina sequencing platforms and real time sequencing analysis. Long term data storage is handled by our 10/40 GbE connected storage arrays. These arrays are currently over 2 petabytes in size. All user data is backed up and stored daily, weekly and monthly. A disaster recovery copy of select data is stored offsite. The HTCF also includes a 600TB high-speed distributed file system that is capable of throughput up to 19GB/s. This, coupled with its 100Gb network backbone, delivering 10Gb of bandwidth to cluster nodes, the HTCF can provide exceptionally high-speed transfer of large amounts of data.","title":"High Throughput Computing Facility @ CGS_SB"},{"location":"globus/","text":"To register a personal HTCF endpoint with Globus. (only needs to be run once) $ globusconnectpersonal -setup Open the link presented in a web browser and follow the on screen instructions. Copy the auth code given at the end of the registration and paste it in the terminal where prompted. To start the endpoint: $ globusconnectpersonal -start -restrict-paths /location/you/want/to/access/files For example, if transferring data to/from /scratch/mylab/mydir/project $ globusconnectpersonal -start -restrict-paths /scratch/mylab/mydir/project For long running transfers, globusconnectpersonal can be run in a screen session or an sbatch job. Once the endpoint is started, transfers can begin. They can be initiated from the globus website, or the globus command line tool (module load py-globus-cli). Please see the Globus documentation to understand both of these methods.","title":"Globus"},{"location":"policies/","text":"Policies WUSTL Computer Use http://wustl.edu/policies/compolicy.html Account Usage As stated in the above WUSTL Policy: \"Do not use the password of others or access files under false identity.\" Accounts and passwords cannot be shared. All users must have their own account. Account Renewal HTCF user accounts are automatically renewed annually from the original activation date unless otherwise instructed. Account Removal Home directories of expired accounts are removed 90 days after expiration. Storage Policies Scratch Data Cleaning In order to ensure top performance of /scratch it is important to clean it regularly to remove stale data. Therefore, the following weekly automated tasks are performed on /scratch: User files on scratch that have not been modified for more than 60 days are garbage collected and placed in a \u201ctrash\u201d location. After 30 days in the trash location, user files are purged from the system. Once purged, there is no way files can be restored. Please ensure that any files you need for more than 60 days are safely copied to an LTS bucket. Garbage-collected files are stored in /scratch/trash/<date_of_collection>/. You can restore your garbage-collected files by moving them out of this directory. A list of your garbage-collected files can be found in /scratch/trash/<date_of_collection>/filelists/<username>. The HTCF is not responsible for data loss from automated scrubs. Labs are responsible for monitoring their files and transferring their data from scratch to long term storage. Data Limits Each member of the HTCF belongs to at least two Unix groups. The primary group is your personal group, having the same name as your HTCF username. The secondary group is the laboratory or similar entity that you are primarily associated with. Policy: Scratch user data limits Size Limit - 2TB Inode Limit (Number of files) - 2,000,000 Example Username: johnsmith To determine your personal scratch usage: $ beegfs-ctl --getquota --uid johnsmith Login Node Policy The HTCF login node is to be used for job composition, software installation, and staging of job data. Any computational processes found running longer than 30 minutes can be terminated. General Availability Effort will be made to keep our resources available. Although the support personnel will do their best to keep the facility running at all times, we cannot guarantee to promptly resolve problems outside office hours, during weekends, and public holidays. Nevertheless, please notify us of whenever they arise. General Maintenance Occasionally, it is necessary as part of maintaining a reliable service to update system software and replace faulty hardware. Sometimes it will be possible to perform these tasks transparently by means of queue reconfiguration in a way that will not disrupt running jobs or interactive use, or significantly inconvenience users. Some tasks however, particularly those affecting storage or login nodes, may require temporary interruption of service. Running Jobs Jobs that improperly perform excessive I/O, or utilize unreserved CPU time will be terminated. Please be accurate when requesting memory, not requesting enough memory will result in your process crashing, requesting too much memory will prevent other users from running jobs.","title":"Policies"},{"location":"policies/#policies","text":"","title":"Policies"},{"location":"policies/#wustl-computer-use","text":"http://wustl.edu/policies/compolicy.html","title":"WUSTL Computer Use"},{"location":"policies/#account-usage","text":"As stated in the above WUSTL Policy: \"Do not use the password of others or access files under false identity.\" Accounts and passwords cannot be shared. All users must have their own account.","title":"Account Usage"},{"location":"policies/#account-renewal","text":"HTCF user accounts are automatically renewed annually from the original activation date unless otherwise instructed.","title":"Account Renewal"},{"location":"policies/#account-removal","text":"Home directories of expired accounts are removed 90 days after expiration.","title":"Account Removal"},{"location":"policies/#storage-policies","text":"","title":"Storage Policies"},{"location":"policies/#scratch-data-cleaning","text":"In order to ensure top performance of /scratch it is important to clean it regularly to remove stale data. Therefore, the following weekly automated tasks are performed on /scratch: User files on scratch that have not been modified for more than 60 days are garbage collected and placed in a \u201ctrash\u201d location. After 30 days in the trash location, user files are purged from the system. Once purged, there is no way files can be restored. Please ensure that any files you need for more than 60 days are safely copied to an LTS bucket. Garbage-collected files are stored in /scratch/trash/<date_of_collection>/. You can restore your garbage-collected files by moving them out of this directory. A list of your garbage-collected files can be found in /scratch/trash/<date_of_collection>/filelists/<username>. The HTCF is not responsible for data loss from automated scrubs. Labs are responsible for monitoring their files and transferring their data from scratch to long term storage.","title":"Scratch Data Cleaning"},{"location":"policies/#data-limits","text":"Each member of the HTCF belongs to at least two Unix groups. The primary group is your personal group, having the same name as your HTCF username. The secondary group is the laboratory or similar entity that you are primarily associated with. Policy: Scratch user data limits Size Limit - 2TB Inode Limit (Number of files) - 2,000,000 Example Username: johnsmith To determine your personal scratch usage: $ beegfs-ctl --getquota --uid johnsmith","title":"Data Limits"},{"location":"policies/#login-node-policy","text":"The HTCF login node is to be used for job composition, software installation, and staging of job data. Any computational processes found running longer than 30 minutes can be terminated.","title":"Login Node Policy"},{"location":"policies/#general-availability","text":"Effort will be made to keep our resources available. Although the support personnel will do their best to keep the facility running at all times, we cannot guarantee to promptly resolve problems outside office hours, during weekends, and public holidays. Nevertheless, please notify us of whenever they arise.","title":"General Availability"},{"location":"policies/#general-maintenance","text":"Occasionally, it is necessary as part of maintaining a reliable service to update system software and replace faulty hardware. Sometimes it will be possible to perform these tasks transparently by means of queue reconfiguration in a way that will not disrupt running jobs or interactive use, or significantly inconvenience users. Some tasks however, particularly those affecting storage or login nodes, may require temporary interruption of service.","title":"General Maintenance"},{"location":"policies/#running-jobs","text":"Jobs that improperly perform excessive I/O, or utilize unreserved CPU time will be terminated. Please be accurate when requesting memory, not requesting enough memory will result in your process crashing, requesting too much memory will prevent other users from running jobs.","title":"Running Jobs"},{"location":"prerequisites/","text":"A firm grasp of the following concepts and technologies are expected for users of the HTCF: Bash Shell https://www.gnu.org/software/bash/manual/html_node/index.html Bash Environment Variables https://linuxhint.com/bash-environment-variables/ Python Virtual Environments https://docs.python.org/3/tutorial/venv.html https://docs.python.org/3/library/venv.html, https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/","title":"Prerequisites"},{"location":"prerequisites/#bash-shell","text":"https://www.gnu.org/software/bash/manual/html_node/index.html","title":"Bash Shell"},{"location":"prerequisites/#bash-environment-variables","text":"https://linuxhint.com/bash-environment-variables/","title":"Bash Environment Variables"},{"location":"prerequisites/#python-virtual-environments","text":"https://docs.python.org/3/tutorial/venv.html https://docs.python.org/3/library/venv.html, https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/","title":"Python Virtual Environments"},{"location":"runningjobs/","text":"Many times you might have a command that you\u2019d like to run on a lot of samples. To run these sequentially, you might have a script like this: #!/bin/bash module load spades spades.py --careful --pe1-1 samp1-r1.fastq --pe1-2 samp1-r2.fastq -o assembly_1 spades.py --careful --pe1-1 samp2-r1.fastq --pe1-2 samp2-r2.fastq -o assembly_2 \u2026 spades.py --careful --pe1-1 samp2000-r1.fastq --pe1-2 samp2000-r2.fastq -o assembly_2000 By submitting this on the HTCF as an \u201carray job\u201d, you have the potential of running each of these commands in parallel (all at the same time) rather than sequentially (one at a time). The best case scenario could be that all 2000 commands finish in the time it takes 1 command to run! More information regarding SLURM job arrays is available at http://slurm.schedmd.com/job_array.html . Run a command or set of commands on file names that are sequentially numbered Input File 1 Input File 2 sample1-r1.fastq sample1-r2.fastq sample2-r1.fastq sample2-r2.fastq \u2026 \u2026 sample2000-r1.fastq sample2000-r2.fastq Step 1: Create an sbatch file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = ${ SLURM_ARRAY_TASK_ID } # Number between 1 and 2000 spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID } Run a command or set of commands on file names that aren\u2019t numbered sequentially Input File 1 Input File 2 sampleAAA-r1.fastq sampleAAA-r2.fastq sampleAAB-r1.fastq sampleAAB-r2.fastq \u2026 \u2026 SampleZZZ-r1.fastq SampleZZZ-r2.fastq Step 1: Create a \u201clookup\u201d file, lookup.txt AAA AAB \u2026 ZZZ Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 2: Create an sbatch file, grabbing the \u201cID\u201d in line $SLURM_ARRAY_TASK_ID from the lookup file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = $( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID } Run a command or set of commands using a complex lookup file Step 1: Create a \u201clookup\u201d file, lookup.txt 5 sampleAAA_R1 0.0001 5000 5 sampleAAA_R2 0.0001 5000 5 sampleBBB_R1 0.0005 1000 5 sampleBBB_R2 0.0005 1000 Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 3: Create an sbatch file, each job is created using the template with information from the lookup file. #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load seqtk read part1 part2 part3 part4 < < ( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) seqtk sample -s ${ part1 } /path/to/file. ${ part2 } .fastq ${ part3 } > /path/to/file. ${ part2 } _ ${ part4 } .fastq","title":"Runningjobs"},{"location":"runningjobs/#run-a-command-or-set-of-commands-on-file-names-that-are-sequentially-numbered","text":"Input File 1 Input File 2 sample1-r1.fastq sample1-r2.fastq sample2-r1.fastq sample2-r2.fastq \u2026 \u2026 sample2000-r1.fastq sample2000-r2.fastq Step 1: Create an sbatch file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = ${ SLURM_ARRAY_TASK_ID } # Number between 1 and 2000 spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID }","title":"Run a command or set of commands on file names that are sequentially numbered"},{"location":"runningjobs/#run-a-command-or-set-of-commands-on-file-names-that-arent-numbered-sequentially","text":"Input File 1 Input File 2 sampleAAA-r1.fastq sampleAAA-r2.fastq sampleAAB-r1.fastq sampleAAB-r2.fastq \u2026 \u2026 SampleZZZ-r1.fastq SampleZZZ-r2.fastq Step 1: Create a \u201clookup\u201d file, lookup.txt AAA AAB \u2026 ZZZ Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 2: Create an sbatch file, grabbing the \u201cID\u201d in line $SLURM_ARRAY_TASK_ID from the lookup file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = $( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID }","title":"Run a command or set of commands on file names that aren\u2019t numbered sequentially"},{"location":"runningjobs/#run-a-command-or-set-of-commands-using-a-complex-lookup-file","text":"Step 1: Create a \u201clookup\u201d file, lookup.txt 5 sampleAAA_R1 0.0001 5000 5 sampleAAA_R2 0.0001 5000 5 sampleBBB_R1 0.0005 1000 5 sampleBBB_R2 0.0005 1000 Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 3: Create an sbatch file, each job is created using the template with information from the lookup file. #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load seqtk read part1 part2 part3 part4 < < ( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) seqtk sample -s ${ part1 } /path/to/file. ${ part2 } .fastq ${ part3 } > /path/to/file. ${ part2 } _ ${ part4 } .fastq","title":"Run a command or set of commands using a complex lookup file"},{"location":"software/","text":"Software The HTCF starts as somewhat of a blank slate for each lab. However, this doesn't mean labs can't get up and running in a matter of minutes! Each lab has its own dedicated space to install and manage software. This reference space is located in /ref/<lab>/software . Software building and installation on the HTCF is primarily self-service. Labs are free to use their /ref software directory to install software using whatever means is most comfortable. At the lab level, use of Spack to install common software is encouraged . Virtual environments can also be used if the software is well suited. Spack Spack is a package management tool designed to support multiple versions and configurations of software on a wide variety of platforms and environments. Note Installation of software on the login node is unsupported. Please build/install software from an interactive job. Tutorial See the official spack tutorial Initialization To create a lab instance of the spack package manager: Download and untar a spack release into /ref/<lab_name>/software Rename (or make a symlink from) /ref/<lab_name>/software/spack-VERSION to /ref/<lab_name>/software/spack . Logout and log back in. This will ensure the spack command is available in the PATH. Installing Software see the official spack documentation Using the software Once spack has built software, the bash shell needs to have the proper environment variables set to access the software. This is accomplished using the spack load command. Spack packages can be \"loaded\" similar to the way modules are loaded. Given a spec , a spack command can be used to generate the appropriate environment variables to \"load\" spack-installed software. To set the environment variables (similar to module load ... ): $ eval $( spack load --sh <spec> ) To unset (unload) these variables: $ eval $( spack unload --sh <spec> ) To simply view the environment variables that would be set without actually setting them: $ spack load --sh <spec> See the official spack documentation for more information on specs. These commands can be placed in an sbatch file to be used in a job. #!/bin/bash eval $( spack load --sh <spec> ) Example Installing biom-format Search for the name of the package: $ spack list biom ==> 7 packages. microbiomeutil py-biomine r-biomart r-biomformat py-biom-format r-biom-utils r-biomartr See what versions are available: $ spack versions py-biom-format ==> Safe versions (already checksummed): 2.1.10 2.1.9 2.1.7 2.1.6 ... Install: $ spack install py-biom-format@2.1.10 Load the software in a job: #!/bin/bash eval $( spack load --sh py-biom-format@2.1.10 ) ... What if the software I want is not available through Spack When needed software is not readily accessible via Spack, there are a few options. Follow the installation instructions from the software creator. Sometimes, this can be very quick and straightforward. Sometimes, this can be very painful. Sometimes, it can be a good idea to pass judgement on the quality of software based on the quality of the installation process and documentation. Create a custom spack package Spack can be a wonderful tool for creating and maintaining software. Plenty of documentation is provided for creating and maintaining custom packages , though a firm understanding of python is needed. R considerations For R software libraries, it's best to install R using Spack, then install R libraries using the R command line interface. This is the best way to ensure the proper installation of the latest (or most appropriate) versions of the R libraries. To create an R library directory that can be used by multiple people, the $R_LIBS_SITE variable can be used. For example, to install R version 4.1.1 and create a shared R library directory: $ spack install r@4.1.1 ... $ export R_LIBS_SITE=/ref/<labname>/software/r-envs/<project_name>/4.1.1 $ mkdir -p $R_LIBS_SITE $ eval $( spack load --sh r@4.1.1 ) $ R ... > install.packages('<pkgname>') > install.packages('<another_pkgname>') > install.packages(c('<and_another_pkgname>', '<one_more_pkgname>')) To use these R libraries in an Rscript job: #!/bin/bash export R_LIBS_SITE=/ref/<labname>/software/r-envs/<project_name>/4.1.1 eval $( spack load --sh r@4.1.1 ) Rscript ........ Manual Installation Sometimes it's just easier to follow the installation steps provided by software creator. If the software depends on other software, it might be that the dependency software could be installed via spack . For example, if a piece of software is not available via Spack, but requires samtools to be installed: # Install samtools via spack $ spack install samtools # load samtools before installation $ eval $( spack load --sh samtools ) (proceed with the manual installation) Remember The best place to install software is in reference storage: /ref/<lab>/software . After manual software installation, it's good practice to then create a module file Manual module files Module files that are manually created go in reference storage in /ref/<lab>/software/modules . The lmod documentation is the best place to learn about creating module files. What about conda? Feel free to use conda if it is required/preferred. We are unable to support Conda. For conda support, please see https://docs.conda.io/en/latest/help-support.html","title":"Software"},{"location":"software/#software","text":"The HTCF starts as somewhat of a blank slate for each lab. However, this doesn't mean labs can't get up and running in a matter of minutes! Each lab has its own dedicated space to install and manage software. This reference space is located in /ref/<lab>/software . Software building and installation on the HTCF is primarily self-service. Labs are free to use their /ref software directory to install software using whatever means is most comfortable. At the lab level, use of Spack to install common software is encouraged . Virtual environments can also be used if the software is well suited.","title":"Software"},{"location":"software/#spack","text":"Spack is a package management tool designed to support multiple versions and configurations of software on a wide variety of platforms and environments. Note Installation of software on the login node is unsupported. Please build/install software from an interactive job.","title":"Spack"},{"location":"software/#tutorial","text":"See the official spack tutorial","title":"Tutorial"},{"location":"software/#initialization","text":"To create a lab instance of the spack package manager: Download and untar a spack release into /ref/<lab_name>/software Rename (or make a symlink from) /ref/<lab_name>/software/spack-VERSION to /ref/<lab_name>/software/spack . Logout and log back in. This will ensure the spack command is available in the PATH.","title":"Initialization"},{"location":"software/#installing-software","text":"see the official spack documentation","title":"Installing Software"},{"location":"software/#using-the-software","text":"Once spack has built software, the bash shell needs to have the proper environment variables set to access the software. This is accomplished using the spack load command. Spack packages can be \"loaded\" similar to the way modules are loaded. Given a spec , a spack command can be used to generate the appropriate environment variables to \"load\" spack-installed software. To set the environment variables (similar to module load ... ): $ eval $( spack load --sh <spec> ) To unset (unload) these variables: $ eval $( spack unload --sh <spec> ) To simply view the environment variables that would be set without actually setting them: $ spack load --sh <spec> See the official spack documentation for more information on specs. These commands can be placed in an sbatch file to be used in a job. #!/bin/bash eval $( spack load --sh <spec> )","title":"Using the software"},{"location":"software/#example","text":"Installing biom-format Search for the name of the package: $ spack list biom ==> 7 packages. microbiomeutil py-biomine r-biomart r-biomformat py-biom-format r-biom-utils r-biomartr See what versions are available: $ spack versions py-biom-format ==> Safe versions (already checksummed): 2.1.10 2.1.9 2.1.7 2.1.6 ... Install: $ spack install py-biom-format@2.1.10 Load the software in a job: #!/bin/bash eval $( spack load --sh py-biom-format@2.1.10 ) ...","title":"Example"},{"location":"software/#what-if-the-software-i-want-is-not-available-through-spack","text":"When needed software is not readily accessible via Spack, there are a few options. Follow the installation instructions from the software creator. Sometimes, this can be very quick and straightforward. Sometimes, this can be very painful. Sometimes, it can be a good idea to pass judgement on the quality of software based on the quality of the installation process and documentation. Create a custom spack package Spack can be a wonderful tool for creating and maintaining software. Plenty of documentation is provided for creating and maintaining custom packages , though a firm understanding of python is needed.","title":"What if the software I want is not available through Spack"},{"location":"software/#r-considerations","text":"For R software libraries, it's best to install R using Spack, then install R libraries using the R command line interface. This is the best way to ensure the proper installation of the latest (or most appropriate) versions of the R libraries. To create an R library directory that can be used by multiple people, the $R_LIBS_SITE variable can be used. For example, to install R version 4.1.1 and create a shared R library directory: $ spack install r@4.1.1 ... $ export R_LIBS_SITE=/ref/<labname>/software/r-envs/<project_name>/4.1.1 $ mkdir -p $R_LIBS_SITE $ eval $( spack load --sh r@4.1.1 ) $ R ... > install.packages('<pkgname>') > install.packages('<another_pkgname>') > install.packages(c('<and_another_pkgname>', '<one_more_pkgname>')) To use these R libraries in an Rscript job: #!/bin/bash export R_LIBS_SITE=/ref/<labname>/software/r-envs/<project_name>/4.1.1 eval $( spack load --sh r@4.1.1 ) Rscript ........","title":"R considerations"},{"location":"software/#manual-installation","text":"Sometimes it's just easier to follow the installation steps provided by software creator. If the software depends on other software, it might be that the dependency software could be installed via spack . For example, if a piece of software is not available via Spack, but requires samtools to be installed: # Install samtools via spack $ spack install samtools # load samtools before installation $ eval $( spack load --sh samtools ) (proceed with the manual installation) Remember The best place to install software is in reference storage: /ref/<lab>/software . After manual software installation, it's good practice to then create a module file","title":"Manual Installation"},{"location":"software/#manual-module-files","text":"Module files that are manually created go in reference storage in /ref/<lab>/software/modules . The lmod documentation is the best place to learn about creating module files.","title":"Manual module files"},{"location":"software/#what-about-conda","text":"Feel free to use conda if it is required/preferred. We are unable to support Conda. For conda support, please see https://docs.conda.io/en/latest/help-support.html","title":"What about conda?"},{"location":"software_old/","text":"Modules Lmod is a Lua based module system that easily handles the MODULEPATH Hierarchical problem. Environment Modules provide a convenient way to dynamically change the users' environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found. Software is handled using lmod . There should be minimal need to modify your .bashrc or .profile unless you're installing software locally to test. Basics Lmod is managed using the command module , using this command without options will show you a list of all available subcommands. ~$ module Modules based on Lua: Version 6.6 2016-10-13 13:28 -05:00 by Robert McLay mclay@tacc.utexas.edu module [options] sub-command [args ...] Help sub-commands: ------------------ help prints this message help module [...] print help message from module(s) A list of software available the command: ~$ module avail ---------------------------------------------- /opt/apps/modules ---------------------------------------------- GD/2.1.1 genometools/1.5.8 pindel/0.2.5b6 R/2.15.3 ghostscript/9.19 pmap/11-25-2010 R/3.1.2 glimmer/3.02b poretools/0.6.0 To load the latest (default) version of module: module load ncbi-blast To specifiy which version of the module you would like to use: module load ncbi-blast/2.2.30+ Be sure to specify which version of the sofware you'd like to use in your scripts to ensure consistent results, as software updates may break pipelines. Software with prerequisites are loaded dynamically, for example: ~$ module load prokka ~$ ml Currently Loaded Modules: 1) aragorn/1.2.36 3) prodigal/2.6.2 5) tbl2asn/24.2 7) hmmer/3.1b1 9) prokka/1.11 2) infernal/1.1.1 4) ncbi-blast/2.2.31+ 6) bio-perl/1.6.923 8) barrnap/0.6 GUI Software As the HTCF is primarily a batch queuing system for high-throughput processing of large amounts of data, GUI application are not directly supported by the HTCF. GUI application installation and setup on the HTCF are left to the end user.","title":"Software old"},{"location":"software_old/#modules","text":"Lmod is a Lua based module system that easily handles the MODULEPATH Hierarchical problem. Environment Modules provide a convenient way to dynamically change the users' environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found. Software is handled using lmod . There should be minimal need to modify your .bashrc or .profile unless you're installing software locally to test.","title":"Modules"},{"location":"software_old/#basics","text":"Lmod is managed using the command module , using this command without options will show you a list of all available subcommands. ~$ module Modules based on Lua: Version 6.6 2016-10-13 13:28 -05:00 by Robert McLay mclay@tacc.utexas.edu module [options] sub-command [args ...] Help sub-commands: ------------------ help prints this message help module [...] print help message from module(s) A list of software available the command: ~$ module avail ---------------------------------------------- /opt/apps/modules ---------------------------------------------- GD/2.1.1 genometools/1.5.8 pindel/0.2.5b6 R/2.15.3 ghostscript/9.19 pmap/11-25-2010 R/3.1.2 glimmer/3.02b poretools/0.6.0 To load the latest (default) version of module: module load ncbi-blast To specifiy which version of the module you would like to use: module load ncbi-blast/2.2.30+ Be sure to specify which version of the sofware you'd like to use in your scripts to ensure consistent results, as software updates may break pipelines. Software with prerequisites are loaded dynamically, for example: ~$ module load prokka ~$ ml Currently Loaded Modules: 1) aragorn/1.2.36 3) prodigal/2.6.2 5) tbl2asn/24.2 7) hmmer/3.1b1 9) prokka/1.11 2) infernal/1.1.1 4) ncbi-blast/2.2.31+ 6) bio-perl/1.6.923 8) barrnap/0.6","title":"Basics"},{"location":"software_old/#gui-software","text":"As the HTCF is primarily a batch queuing system for high-throughput processing of large amounts of data, GUI application are not directly supported by the HTCF. GUI application installation and setup on the HTCF are left to the end user.","title":"GUI Software"},{"location":"release-notes/2021-q1/","text":"Q1 - 2021 Changes The beginning of 2021 will see quite a few (pretty cool, we think) changes within the HTCF. Much effort has been made to ease the transitions that are necessary to make these changes. Please contact us with any questions or concerns. What's changing? There are essentially 4 changes (with more detail and schedule further down): OS Upgrade While the HTCF backend servers have been continually upgraded over the years, the cluster nodes and login server are long overdue for an OS upgrade. Note Many of the current software modules rely on system libraries. These libraries will be upgraded when the OS is upgraded. It it likely that some software packages will have errors due to the change in system libraries. Don't worry though, we've got a solution for that.... Self-Service Software As the HTCF user base grows, so do the software installation needs. Maintaining the software and modules for all labs can leave a web of dependency, version, and architecture conflicts that are difficult to untangle. The most sensible, sustainable, and flexible approach going forward is for HTCF support to move over to the passenger seat and leave the driving to the labs. Labs should now feel empowered to install and manage their own software (with us in the passenger seat to help navigate). Infrastructure is in place to ease in this, including an initial 1TB of space per lab for software storage called /ref. Speaking of which.... /ref A new form of LTS storage called /ref will be introduced. /ref is storage space for software and reference databases (think NCBI databases or software-provided reference sequences). Each lab will be given an initial 1TB of reference space, this space can be expanded at LTS prices. Speaking of new forms of LTS.... LTOS Object Storage LTS as been quietly available for a while, but now it's official. This is the cloud storage form of local LTS ( local meaning high speed and hosted within the HTCF) that uses the Amazon S3 API for access. It's well suited for large amounts of data that don't change much (e.g. raw sequencing runs). Unlike traditional LTS : LTOS buckets don't have a size limit They're available from each node no more login server copying back and forth! Aggregate transfer rates can be much faster than traditional /lts. Before you get too excited, there's a learning curve...because it's more like cloud storage, you won't find it in a traditional filesystem directory. So cp , mv , and rsync will not help you. With the right use case though, we're confident that the learning curve will be well worth it! OS Upgrade All nodes (including the login server) will be upgraded from Ubuntu 16.04 to Ubuntu 20.04. Warning Many of the current software modules rely on system libraries. These libraries will be upgraded when the OS is upgraded. It it likely that some software packages will have errors due to the change in system libraries. Date Change % nodes containing upgraded OS Jan 11 New slurm partition beta will be made available for trying out jobs on a small number of nodes that use the upgraded OS. To use, add: #SBATCH -p beta to your sbatch files 5% Jan 18 Login Server will be upgraded to new OS 5% Jan 18 - 25 Rolling OS upgrades of idle nodes (up to 50% of nodes) will continue. 5% - 50% Feb 1 Slurm partition general will be renamed legacy and beta will be renamed to general . 50% Feb 1 Slurm partition general will become the default interactive and batch job partition. 50% Feb 1 - 15 Rolling OS upgrades of idle nodes (up to 95% of nodes) will continue. 50% - 95% Mar 8 All nodes upgraded Self-service software and modules The system-wide software and modules in directories /opt/apps and /opt/htcf will be frozen (read-only) and use of software in these directories is deprecated. Replacing these directories will be a form of LTS call /ref, lab reference space located in /ref/<lab>/software and /ref/<lab>/modulefiles . Each lab will receive 1TB of space in /ref for storage of reference material including software and reference sequence databases. More information about /ref can be found here. Software installation will now be self-service. To aid in this, documentation has been added and infrastructue is in place for lab-managed spack installations . Thousands of packages are available for installation using spack . Help We are here to help with the migration from the deprecated modules system to your labs' new /ref software. Rest assured that the deprecated software will not be removed until migration is complete. Date Change Jan 11 Each lab given 1TB of /ref space which will be availabe on all OS-upgraded nodes . To begin testing: srun --mem=X000 -c X -p beta --pty /bin/bash -l Jan 18 /opt/apps and /opt/htcf will go read-only to encourage use of /ref Jan 25 modules loaded from /opt/apps and /opt/htcf will contain a Deprecation Warning to encourage use of /ref Mar 1 - 8 modules for software in /opt/apps and /opt/htcf will be incrementally removed Mar 8 /opt/apps and /opt/htcf directories will be decommissioned. /ref Note Keep in mind, while /ref is a form of LTS, it is not /lts (e.g. it isn't backed up) Date Change Jan 11 Each lab given 1TB of /ref space which will be availabe on all OS-upgraded nodes to begin testing: srun --mem=X000 -c X -p beta --pty /bin/bash -l Feb 1 The /scratch/ref data directory will be renamed /scratch/old-ref to encourage use of /ref/<lab>/data/ instead. Mar 8 /scratch/old-ref directory will be decommissioned. LTOS Date Change Jan 11 LTOS open for business Feb 1 Labs with traditional /lts buckets >= 15TB will be encouraged to consider LTOS for some of their bulky data (such as raw sequence data)","title":"Q1 - 2021 Changes"},{"location":"release-notes/2021-q1/#q1-2021-changes","text":"The beginning of 2021 will see quite a few (pretty cool, we think) changes within the HTCF. Much effort has been made to ease the transitions that are necessary to make these changes. Please contact us with any questions or concerns.","title":"Q1 - 2021 Changes"},{"location":"release-notes/2021-q1/#whats-changing","text":"There are essentially 4 changes (with more detail and schedule further down): OS Upgrade While the HTCF backend servers have been continually upgraded over the years, the cluster nodes and login server are long overdue for an OS upgrade. Note Many of the current software modules rely on system libraries. These libraries will be upgraded when the OS is upgraded. It it likely that some software packages will have errors due to the change in system libraries. Don't worry though, we've got a solution for that.... Self-Service Software As the HTCF user base grows, so do the software installation needs. Maintaining the software and modules for all labs can leave a web of dependency, version, and architecture conflicts that are difficult to untangle. The most sensible, sustainable, and flexible approach going forward is for HTCF support to move over to the passenger seat and leave the driving to the labs. Labs should now feel empowered to install and manage their own software (with us in the passenger seat to help navigate). Infrastructure is in place to ease in this, including an initial 1TB of space per lab for software storage called /ref. Speaking of which.... /ref A new form of LTS storage called /ref will be introduced. /ref is storage space for software and reference databases (think NCBI databases or software-provided reference sequences). Each lab will be given an initial 1TB of reference space, this space can be expanded at LTS prices. Speaking of new forms of LTS.... LTOS Object Storage LTS as been quietly available for a while, but now it's official. This is the cloud storage form of local LTS ( local meaning high speed and hosted within the HTCF) that uses the Amazon S3 API for access. It's well suited for large amounts of data that don't change much (e.g. raw sequencing runs). Unlike traditional LTS : LTOS buckets don't have a size limit They're available from each node no more login server copying back and forth! Aggregate transfer rates can be much faster than traditional /lts. Before you get too excited, there's a learning curve...because it's more like cloud storage, you won't find it in a traditional filesystem directory. So cp , mv , and rsync will not help you. With the right use case though, we're confident that the learning curve will be well worth it!","title":"What's changing?"},{"location":"release-notes/2021-q1/#os-upgrade","text":"All nodes (including the login server) will be upgraded from Ubuntu 16.04 to Ubuntu 20.04. Warning Many of the current software modules rely on system libraries. These libraries will be upgraded when the OS is upgraded. It it likely that some software packages will have errors due to the change in system libraries. Date Change % nodes containing upgraded OS Jan 11 New slurm partition beta will be made available for trying out jobs on a small number of nodes that use the upgraded OS. To use, add: #SBATCH -p beta to your sbatch files 5% Jan 18 Login Server will be upgraded to new OS 5% Jan 18 - 25 Rolling OS upgrades of idle nodes (up to 50% of nodes) will continue. 5% - 50% Feb 1 Slurm partition general will be renamed legacy and beta will be renamed to general . 50% Feb 1 Slurm partition general will become the default interactive and batch job partition. 50% Feb 1 - 15 Rolling OS upgrades of idle nodes (up to 95% of nodes) will continue. 50% - 95% Mar 8 All nodes upgraded","title":"OS Upgrade"},{"location":"release-notes/2021-q1/#self-service-software-and-modules","text":"The system-wide software and modules in directories /opt/apps and /opt/htcf will be frozen (read-only) and use of software in these directories is deprecated. Replacing these directories will be a form of LTS call /ref, lab reference space located in /ref/<lab>/software and /ref/<lab>/modulefiles . Each lab will receive 1TB of space in /ref for storage of reference material including software and reference sequence databases. More information about /ref can be found here. Software installation will now be self-service. To aid in this, documentation has been added and infrastructue is in place for lab-managed spack installations . Thousands of packages are available for installation using spack . Help We are here to help with the migration from the deprecated modules system to your labs' new /ref software. Rest assured that the deprecated software will not be removed until migration is complete. Date Change Jan 11 Each lab given 1TB of /ref space which will be availabe on all OS-upgraded nodes . To begin testing: srun --mem=X000 -c X -p beta --pty /bin/bash -l Jan 18 /opt/apps and /opt/htcf will go read-only to encourage use of /ref Jan 25 modules loaded from /opt/apps and /opt/htcf will contain a Deprecation Warning to encourage use of /ref Mar 1 - 8 modules for software in /opt/apps and /opt/htcf will be incrementally removed Mar 8 /opt/apps and /opt/htcf directories will be decommissioned.","title":"Self-service software and modules"},{"location":"release-notes/2021-q1/#ref","text":"Note Keep in mind, while /ref is a form of LTS, it is not /lts (e.g. it isn't backed up) Date Change Jan 11 Each lab given 1TB of /ref space which will be availabe on all OS-upgraded nodes to begin testing: srun --mem=X000 -c X -p beta --pty /bin/bash -l Feb 1 The /scratch/ref data directory will be renamed /scratch/old-ref to encourage use of /ref/<lab>/data/ instead. Mar 8 /scratch/old-ref directory will be decommissioned.","title":"/ref"},{"location":"release-notes/2021-q1/#ltos","text":"Date Change Jan 11 LTOS open for business Feb 1 Labs with traditional /lts buckets >= 15TB will be encouraged to consider LTOS for some of their bulky data (such as raw sequence data)","title":"LTOS"},{"location":"storage/","text":"Storage The HTCF provides five types of storage: HDS Home Directory Storage (HDS) can be used to store scripts, development tools, etc. Home directories are located in /home/<WUSTLKEY_ID> and are available on all nodes. HDS is kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. Note Home directory space is not a high-speed resource like /scratch space. Please keep home directory access to a minimum in batch jobs. By default, home directories will not be readable or writeable by other users of HTCF. Feel free to change this default, if desired. To check home directory usage: $ du -sh $HOME LTS Long Term Storage (LTS) is lab project space to store raw sequencing and completed data, the directories are not available on nodes for computational use. It is available in terabyte increments billed monthly. It is kept on fault-tolerant storage with snapshops. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. To check LTS usage: $ df -h /lts/<lab_name>/<bucket_name> LTOS Coming Soon REF /ref is storage space for software and reference databases (such as NCBI databases or software-provided reference sequences). Each lab has an initial 1TB of reference space, and this space can be expanded at LTS prices. Note /ref is not currently backed up, therefore Any data in /ref that cannot be recreated should be copied to long term storage (LTS) for safe-keeping. /ref \u251c\u2500\u2500 <lab> \u251c\u2500\u2500 data \u2514\u2500\u2500 software The data directory is well suited for modestly sized reference data such as NCBI blast databases . Larger datasets (> 500GB) are probably better suited for Long Term Object Storage The software directory is suited for software installation using tools such as spack HTS High Throughput Storage ( /scratch ) is a distrubuted file system able to handle tens of GBs/sec of total throughput. This storage is temporary scratch space and is not backed up. Once data is removed from /scratch, it cannot be recovered. Data stored in /scratch is subject to the Scratch Data Cleaning Policy . Jobs utilize this space for inputs and outputs to provide the best performance possible. Running jobs that read/write from the home directory will cause slowness and login issues for all users. Note Users will be asked to clean up older files on /scratch if it is needed to improve system performance. The best use of the HTS is to use a workflow similar to the following: Copy starting (raw) data from LTS to HTS. Submit jobs to the cluster that process the data, creating intermediate and/or finished data. Copy the finished data (and job files used to create that data) over to LTS. Remove all working data from HTS Results that are generated on this storage need to be promptly copied to LTS. There is a quota of 2TB per user in /scratch to prevent the filesystem from filling up. At >85% /scratch can become very slow. To check the amount of space being used, use the following command: $ beegfs-ctl --getquota --uid $USER Quota Increase Requests Note A quota increase is not garaunteed. If excess capacity is not available, a quota increase cannot be granted. If excess capacity is available. A temporary increase in the scratch quota can be requested. To request more scratch space, email following information: Reason for the increase Amount of additional space Duration additional space will be required Recommendations No important source code, scripts, libraries, executables should be kept in /scratch Do not make symlinks from the home directory to folders in /scratch Sharing Files Publicly Globus can be used to share data from LTS or Home directories. The Globus \"Collection\" is called \"HTCF Home Directories\". Please see the globus documentation for more information For long term hosting of publicly accessible data, please contact WUSTL IT. Copying Files Using Rsync Using rsync to transfer to scratch and LTS is recommended. Rsync can resume failed copies, be re-run to ensure all of the data has been transferred, and will also transfer incremental changes. This will save a substantial amount of time if it is necessary to verify that all files have been successfully copied. When using this command, please note that the absense of a trailing slash means the directory, with a trailing slash means the contents of that directory. Here are a few examples: More info... Disk Quota Exceeded Errors If a disk quota exceeded error messages is encountered, please check each storage location to ensure there is enough disk space available.","title":"Storage"},{"location":"storage/#storage","text":"The HTCF provides five types of storage:","title":"Storage"},{"location":"storage/#hds","text":"Home Directory Storage (HDS) can be used to store scripts, development tools, etc. Home directories are located in /home/<WUSTLKEY_ID> and are available on all nodes. HDS is kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. Note Home directory space is not a high-speed resource like /scratch space. Please keep home directory access to a minimum in batch jobs. By default, home directories will not be readable or writeable by other users of HTCF. Feel free to change this default, if desired. To check home directory usage: $ du -sh $HOME","title":"HDS"},{"location":"storage/#lts","text":"Long Term Storage (LTS) is lab project space to store raw sequencing and completed data, the directories are not available on nodes for computational use. It is available in terabyte increments billed monthly. It is kept on fault-tolerant storage with snapshops. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. To check LTS usage: $ df -h /lts/<lab_name>/<bucket_name>","title":"LTS"},{"location":"storage/#ltos","text":"Coming Soon","title":"LTOS"},{"location":"storage/#ref","text":"/ref is storage space for software and reference databases (such as NCBI databases or software-provided reference sequences). Each lab has an initial 1TB of reference space, and this space can be expanded at LTS prices. Note /ref is not currently backed up, therefore Any data in /ref that cannot be recreated should be copied to long term storage (LTS) for safe-keeping. /ref \u251c\u2500\u2500 <lab> \u251c\u2500\u2500 data \u2514\u2500\u2500 software The data directory is well suited for modestly sized reference data such as NCBI blast databases . Larger datasets (> 500GB) are probably better suited for Long Term Object Storage The software directory is suited for software installation using tools such as spack","title":"REF"},{"location":"storage/#hts","text":"High Throughput Storage ( /scratch ) is a distrubuted file system able to handle tens of GBs/sec of total throughput. This storage is temporary scratch space and is not backed up. Once data is removed from /scratch, it cannot be recovered. Data stored in /scratch is subject to the Scratch Data Cleaning Policy . Jobs utilize this space for inputs and outputs to provide the best performance possible. Running jobs that read/write from the home directory will cause slowness and login issues for all users. Note Users will be asked to clean up older files on /scratch if it is needed to improve system performance. The best use of the HTS is to use a workflow similar to the following: Copy starting (raw) data from LTS to HTS. Submit jobs to the cluster that process the data, creating intermediate and/or finished data. Copy the finished data (and job files used to create that data) over to LTS. Remove all working data from HTS Results that are generated on this storage need to be promptly copied to LTS. There is a quota of 2TB per user in /scratch to prevent the filesystem from filling up. At >85% /scratch can become very slow. To check the amount of space being used, use the following command: $ beegfs-ctl --getquota --uid $USER","title":"HTS"},{"location":"storage/#quota-increase-requests","text":"Note A quota increase is not garaunteed. If excess capacity is not available, a quota increase cannot be granted. If excess capacity is available. A temporary increase in the scratch quota can be requested. To request more scratch space, email following information: Reason for the increase Amount of additional space Duration additional space will be required","title":"Quota Increase Requests"},{"location":"storage/#recommendations","text":"No important source code, scripts, libraries, executables should be kept in /scratch Do not make symlinks from the home directory to folders in /scratch","title":"Recommendations"},{"location":"storage/#sharing-files-publicly","text":"Globus can be used to share data from LTS or Home directories. The Globus \"Collection\" is called \"HTCF Home Directories\". Please see the globus documentation for more information For long term hosting of publicly accessible data, please contact WUSTL IT.","title":"Sharing Files Publicly"},{"location":"storage/#copying-files-using-rsync","text":"Using rsync to transfer to scratch and LTS is recommended. Rsync can resume failed copies, be re-run to ensure all of the data has been transferred, and will also transfer incremental changes. This will save a substantial amount of time if it is necessary to verify that all files have been successfully copied. When using this command, please note that the absense of a trailing slash means the directory, with a trailing slash means the contents of that directory. Here are a few examples: More info...","title":"Copying Files Using Rsync"},{"location":"storage/#disk-quota-exceeded-errors","text":"If a disk quota exceeded error messages is encountered, please check each storage location to ensure there is enough disk space available.","title":"Disk Quota Exceeded Errors"},{"location":"storage/compare/","text":"Storage Comparison .md-typeset table:not([class]) th:nth-child(2) { text-align: center; background-color: #4473c5; } .md-typeset table:not([class]) th:nth-child(3) { background-color: #70ad46; } .md-typeset table:not([class]) th:nth-child(4) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(4), .md-typeset table:not([class]) th:nth-child(6) { color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(5) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(6) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(6) :text { visibility: hidden; } .md-typeset table:not([class]) thead th { font-size: 1rem; } .md-typeset table:not([class]) thead th, .md-typeset table:not([class]) tbody tr:nth-child(1) td, .md-typeset table:not([class]) td:nth-child(1) { font-weight: bold; } .md-typeset table:not([class]) td:nth-child(2) { background-color: #dae3f4; } .md-typeset table:not([class]) td:nth-child(3) { background-color: #e2f0d9; } .md-typeset table:not([class]) td:nth-child(4) { background-color: #f4b184; } .md-typeset table:not([class]) td:nth-child(5) { background-color: #f8cbac; } .md-typeset table:not([class]) td:nth-child(6) { background-color: #fbe5d7; } .md-typeset table:not([class]) tr:nth-child(1) td:nth-child(2) { background-color: #8fabda; } .md-typeset table:not([class]) tr:nth-child(1) td:nth-child(3) { background-color: #a8d18d; } table.storage thead tr.sub th.hts { background-color: #a8d18d; } table.storage thead tr.sub th.lts { background-color: #f4b184; } table.storage thead tr.sub th.ltos { background-color: #f7ccac; } table.storage thead tr.sub th.ref { background-color: #fbe5d7; } table.storage tbody td.lts { background-color: #f4b184; } .md-sidebar { display: none; } .md-content { max-width: 100%; } Home HTS LTS LTS LTS Home Directory (/home) Scratch Space (/scratch) Filesystem (/lts) Object Store (LTOS) Reference (/ref) Purpose - environment customizations and scripts - development and storage of small personal project TEMPORARY STORAGE FOR: raw/initial data needed for processing by running/pending jobs intermediate data generated by running jobs final processed data prior to moving data to safer location (LTS) after jobs complete Long Term Storage For: raw/initial data such as sequencer data finished (fully processed) data documentation describing how to reproduce fully processed data from initial data Long Term Storage For: raw/initial data such as sequencer data finished (fully processed) data documentation describing how to reproduce fully processed data from initial data Long Term Storage For: Software tools needed by jobs for the processing of data reference data such as reference genomes and NCBI databases Initial Size 20G 2TB per account - - 1TB per lab Increase Cost NA $7.77/TB/Month $7.77/TB/Month $7.77/TB/Month Size Limit 20G Temporary increase available as resources allow. Please provide # TBs needed and duration of need 10 TB per bucket No limit on # of buckets - - Access from All HTCF Nodes All HTCF Nodes Login Node Only All HTCF Nodes All HTCF Nodes Access Type Standard Filesystem Standard Filesystem Standard Filesystem /lts/<lab>/<bucket> HTTP interface compatible with (but not using) Amazon S3 API Standard Filesystem /ref/<lab>/data /ref/<lab>/software Est. Access Speed Slow 10+ GB/s (aggregate) 200 MB/s 1+ GB/s (aggregate) 100 MB/s (aggregate) Backup Policy Onsite daily/weekly/monthly snapshots as resources allow Offsite backup daily NO BACKUPS Onsite daily/weekly/monthly snapshots as resources allow. Offsite backup daily. mirrored offsite. User customizable: versioning of objects, schedule removal of old objects NO BACKUPS Cleaning Policy - See the scratch data cleaning policy - - -","title":"Storage Comparison"},{"location":"storage/compare/#storage-comparison","text":".md-typeset table:not([class]) th:nth-child(2) { text-align: center; background-color: #4473c5; } .md-typeset table:not([class]) th:nth-child(3) { background-color: #70ad46; } .md-typeset table:not([class]) th:nth-child(4) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(4), .md-typeset table:not([class]) th:nth-child(6) { color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(5) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(6) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(6) :text { visibility: hidden; } .md-typeset table:not([class]) thead th { font-size: 1rem; } .md-typeset table:not([class]) thead th, .md-typeset table:not([class]) tbody tr:nth-child(1) td, .md-typeset table:not([class]) td:nth-child(1) { font-weight: bold; } .md-typeset table:not([class]) td:nth-child(2) { background-color: #dae3f4; } .md-typeset table:not([class]) td:nth-child(3) { background-color: #e2f0d9; } .md-typeset table:not([class]) td:nth-child(4) { background-color: #f4b184; } .md-typeset table:not([class]) td:nth-child(5) { background-color: #f8cbac; } .md-typeset table:not([class]) td:nth-child(6) { background-color: #fbe5d7; } .md-typeset table:not([class]) tr:nth-child(1) td:nth-child(2) { background-color: #8fabda; } .md-typeset table:not([class]) tr:nth-child(1) td:nth-child(3) { background-color: #a8d18d; } table.storage thead tr.sub th.hts { background-color: #a8d18d; } table.storage thead tr.sub th.lts { background-color: #f4b184; } table.storage thead tr.sub th.ltos { background-color: #f7ccac; } table.storage thead tr.sub th.ref { background-color: #fbe5d7; } table.storage tbody td.lts { background-color: #f4b184; } .md-sidebar { display: none; } .md-content { max-width: 100%; } Home HTS LTS LTS LTS Home Directory (/home) Scratch Space (/scratch) Filesystem (/lts) Object Store (LTOS) Reference (/ref) Purpose - environment customizations and scripts - development and storage of small personal project TEMPORARY STORAGE FOR: raw/initial data needed for processing by running/pending jobs intermediate data generated by running jobs final processed data prior to moving data to safer location (LTS) after jobs complete Long Term Storage For: raw/initial data such as sequencer data finished (fully processed) data documentation describing how to reproduce fully processed data from initial data Long Term Storage For: raw/initial data such as sequencer data finished (fully processed) data documentation describing how to reproduce fully processed data from initial data Long Term Storage For: Software tools needed by jobs for the processing of data reference data such as reference genomes and NCBI databases Initial Size 20G 2TB per account - - 1TB per lab Increase Cost NA $7.77/TB/Month $7.77/TB/Month $7.77/TB/Month Size Limit 20G Temporary increase available as resources allow. Please provide # TBs needed and duration of need 10 TB per bucket No limit on # of buckets - - Access from All HTCF Nodes All HTCF Nodes Login Node Only All HTCF Nodes All HTCF Nodes Access Type Standard Filesystem Standard Filesystem Standard Filesystem /lts/<lab>/<bucket> HTTP interface compatible with (but not using) Amazon S3 API Standard Filesystem /ref/<lab>/data /ref/<lab>/software Est. Access Speed Slow 10+ GB/s (aggregate) 200 MB/s 1+ GB/s (aggregate) 100 MB/s (aggregate) Backup Policy Onsite daily/weekly/monthly snapshots as resources allow Offsite backup daily NO BACKUPS Onsite daily/weekly/monthly snapshots as resources allow. Offsite backup daily. mirrored offsite. User customizable: versioning of objects, schedule removal of old objects NO BACKUPS Cleaning Policy - See the scratch data cleaning policy - - -","title":"Storage Comparison"},{"location":"storage/ltos/","text":"Long Term Object Storage Long Term Object Storage (LTOS) is an architecture that manages data as objects , as opposed to traditional LTS which uses file systems and block storage. Access to LTOS is REST -based (HTTP). Although LTOS uses a subset of the Amazon s3 API , LTOS data is stored internally , not in Amazon. Unlike LTS, which is only accessible from the login server, LTOS is accessible from all HTCF nodes. Purpose LTOS can be a good alternative to LTS any time the data in question doesn't need to be heavily manipulated or modified. Good Candidates for LTOS: Raw sequence data and finished analysis data Archived or rarely referenced data such as alumni files. Data that is not often modified, once created. Not Good Candidates for LTOS: scripts in use or under development software intermediate files generated during job processing Using LTOS Software Tools CLI The two most common command line tools for accessing LTOS are s3cmd and aws-cli. s3cmd First a configuration files needs to be created Workflow Putting files in LTOS Examples Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Long Term Object Storage"},{"location":"storage/ltos/#long-term-object-storage","text":"Long Term Object Storage (LTOS) is an architecture that manages data as objects , as opposed to traditional LTS which uses file systems and block storage. Access to LTOS is REST -based (HTTP). Although LTOS uses a subset of the Amazon s3 API , LTOS data is stored internally , not in Amazon. Unlike LTS, which is only accessible from the login server, LTOS is accessible from all HTCF nodes.","title":"Long Term Object Storage"},{"location":"storage/ltos/#purpose","text":"LTOS can be a good alternative to LTS any time the data in question doesn't need to be heavily manipulated or modified. Good Candidates for LTOS: Raw sequence data and finished analysis data Archived or rarely referenced data such as alumni files. Data that is not often modified, once created. Not Good Candidates for LTOS: scripts in use or under development software intermediate files generated during job processing","title":"Purpose"},{"location":"storage/ltos/#using-ltos","text":"","title":"Using LTOS"},{"location":"storage/ltos/#software-tools","text":"","title":"Software Tools"},{"location":"storage/ltos/#cli","text":"The two most common command line tools for accessing LTOS are s3cmd and aws-cli.","title":"CLI"},{"location":"storage/ltos/#s3cmd","text":"First a configuration files needs to be created","title":"s3cmd"},{"location":"storage/ltos/#workflow","text":"","title":"Workflow"},{"location":"storage/ltos/#putting-files-in-ltos","text":"","title":"Putting files in LTOS"},{"location":"storage/ltos/#examples","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Examples"},{"location":"using/","text":"Your HTCF Account Once your software is set up and your data is placed in the proper storage location you are ready to get down to work. Work is done on the HTCF via use of the Workload Management System (or Job Scheduler). Slurm Scheduler Overview Here HTCF Layout HERE (partitions) batch jobs and monitoring of running batch jobs interactive jobs job accounting","title":"Your HTCF Account"},{"location":"using/#your-htcf-account","text":"Once your software is set up and your data is placed in the proper storage location you are ready to get down to work. Work is done on the HTCF via use of the Workload Management System (or Job Scheduler). Slurm Scheduler Overview Here HTCF Layout HERE (partitions) batch jobs and monitoring of running batch jobs interactive jobs job accounting","title":"Your HTCF Account"},{"location":"using/accounting/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Accounting"},{"location":"using/batch/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Batch"},{"location":"using/getstarted/","text":"Your HTCF Account Account Creation To request a user account on the HTCF, please send an email containing the WUSTLKey username and department ID (Workday 'CC' number), for billing purposes. Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account. Logging In WUSTLKey credentials are used for authentication ( http://wustlkey.wustl.edu/ ) The login server, login.htcf.wustl.edu is accessible via ssh. Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account. First things first... Before using the HTCF, it's important to read through and understand: The HTCF Storage Using software on the HTCF Data & Data Storage Home Directories Home directories are a small, fixed amount of storage per account. They are kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. More info... Long Term Storage LTS is used to store raw and \"finished\" project data. The LTS directories are not available on the cluster nodes. It is kept on fault-tolerant storage with snapshops. More info... High Throughput Storage High Throughput Storage is a distrubuted file system able to handle tens of GBs/sec of total throughput. More info... GUI Software Note As the HTCF is primarily a batch queuing system for high-throughput processing of data, use of GUI applications are not directly supported. While use of GUI applications is possible using X forwarding, this can sometimes require significant desktop preparation and configuration which is beyond the scope of support. Jobs Resources The number of CPUs and MBs of RAM per node can be found using the Slurm sinfo command: $ sinfo -N -p general -o '%n %c %m' Interactive Interactive sessions are for running interactive scripts, vizualization, any tasks that are too computational intensive to run on the login node not submitted via sbatch. The defaults are: 1 CPU core, 1 GB RAM, and a time limit of 8 hours. Note The HTCF is primarily a batch queuing system. Interactive jobs are meant to function as daily workspaces. Because interactive jobs are by their nature, inefficient, they are not meant to be running continuously for more than 1 day. When using interactive tools such as rstudio or jupyter, please make sure the jobs are using the \"interactive\" queue (using sbatch/srun parameters -J interactive -p interactive ) Jobs using interactive tools that are not in the interactive queue will be subject to cancellation in order to free up resources for batch jobs. Tools such as Rscript can be used to run R programs in a batch fashion. It appears that jupyter notebooks can also be run in a batch fashion . Thanks for helping to ensure fairness for all folks on the HTCF. An interactive session can be started using the Slurm srun command: $ srun --mem=<MBs> --cpus-per-task=<num> -J interactive -p interactive --pty /bin/bash -l Batch Job Submission Determine resources Create Job File Create sbatch file with required resources Submit Monitor Workflow Jobs typically follow a generic workflow. Preprocessed Raw Data Enters LTS Raw Data is copied to scratch for processing Post processed data is copied to LTS Intermediate data generated in Step 2 is removed Sbatch Examples Create a job script (myjob.sbatch): #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem=1G eval $( spack load --sh <program> ) program /scratch/lab/files/ABC.fasta /scratch/lab/files/ABC.out Submit the sbatch script: $ sbatch myjob.sbatch View the job in the queue: $ squeue GPUs The HTCF currently has a small number of GPUs. A GPU is accessible using the following slurm parameters: #SBATCH -p gpu #SBATCH --gres=gpu","title":"Your HTCF Account"},{"location":"using/getstarted/#your-htcf-account","text":"","title":"Your HTCF Account"},{"location":"using/getstarted/#account-creation","text":"To request a user account on the HTCF, please send an email containing the WUSTLKey username and department ID (Workday 'CC' number), for billing purposes. Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account.","title":"Account Creation"},{"location":"using/getstarted/#logging-in","text":"WUSTLKey credentials are used for authentication ( http://wustlkey.wustl.edu/ ) The login server, login.htcf.wustl.edu is accessible via ssh. Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account.","title":"Logging In"},{"location":"using/getstarted/#first-things-first","text":"Before using the HTCF, it's important to read through and understand: The HTCF Storage Using software on the HTCF","title":"First things first..."},{"location":"using/getstarted/#data-data-storage","text":"","title":"Data &amp; Data Storage"},{"location":"using/getstarted/#home-directories","text":"Home directories are a small, fixed amount of storage per account. They are kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. More info...","title":"Home Directories"},{"location":"using/getstarted/#long-term-storage","text":"LTS is used to store raw and \"finished\" project data. The LTS directories are not available on the cluster nodes. It is kept on fault-tolerant storage with snapshops. More info...","title":"Long Term Storage"},{"location":"using/getstarted/#high-throughput-storage","text":"High Throughput Storage is a distrubuted file system able to handle tens of GBs/sec of total throughput. More info...","title":"High Throughput Storage"},{"location":"using/getstarted/#gui-software","text":"Note As the HTCF is primarily a batch queuing system for high-throughput processing of data, use of GUI applications are not directly supported. While use of GUI applications is possible using X forwarding, this can sometimes require significant desktop preparation and configuration which is beyond the scope of support.","title":"GUI Software"},{"location":"using/getstarted/#jobs","text":"","title":"Jobs"},{"location":"using/getstarted/#resources","text":"The number of CPUs and MBs of RAM per node can be found using the Slurm sinfo command: $ sinfo -N -p general -o '%n %c %m'","title":"Resources"},{"location":"using/getstarted/#interactive","text":"Interactive sessions are for running interactive scripts, vizualization, any tasks that are too computational intensive to run on the login node not submitted via sbatch. The defaults are: 1 CPU core, 1 GB RAM, and a time limit of 8 hours. Note The HTCF is primarily a batch queuing system. Interactive jobs are meant to function as daily workspaces. Because interactive jobs are by their nature, inefficient, they are not meant to be running continuously for more than 1 day. When using interactive tools such as rstudio or jupyter, please make sure the jobs are using the \"interactive\" queue (using sbatch/srun parameters -J interactive -p interactive ) Jobs using interactive tools that are not in the interactive queue will be subject to cancellation in order to free up resources for batch jobs. Tools such as Rscript can be used to run R programs in a batch fashion. It appears that jupyter notebooks can also be run in a batch fashion . Thanks for helping to ensure fairness for all folks on the HTCF. An interactive session can be started using the Slurm srun command: $ srun --mem=<MBs> --cpus-per-task=<num> -J interactive -p interactive --pty /bin/bash -l","title":"Interactive"},{"location":"using/getstarted/#batch-job-submission","text":"Determine resources Create Job File Create sbatch file with required resources Submit Monitor","title":"Batch Job Submission"},{"location":"using/getstarted/#workflow","text":"Jobs typically follow a generic workflow. Preprocessed Raw Data Enters LTS Raw Data is copied to scratch for processing Post processed data is copied to LTS Intermediate data generated in Step 2 is removed","title":"Workflow"},{"location":"using/getstarted/#sbatch-examples","text":"Create a job script (myjob.sbatch): #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem=1G eval $( spack load --sh <program> ) program /scratch/lab/files/ABC.fasta /scratch/lab/files/ABC.out Submit the sbatch script: $ sbatch myjob.sbatch View the job in the queue: $ squeue","title":"Sbatch Examples"},{"location":"using/getstarted/#gpus","text":"The HTCF currently has a small number of GPUs. A GPU is accessible using the following slurm parameters: #SBATCH -p gpu #SBATCH --gres=gpu","title":"GPUs"},{"location":"using/interactive/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Interactive"},{"location":"using/jupyter/","text":"#!/bin/bash #SBATCH --mem=1G #SBATCH --cpus-per-task=1 #SBATCH --time 8:00:00 #SBATCH --output jupyter-lab-%J.out . env/bin/activate unset XDG_RUNTIME_DIR port=$(shuf -i9000-9999 -n1) echo -e \" Run in a new local terminal window to create an SSH tunnel to $host ----------------------------------------------------------------- ssh -N -L $port:$host:$port $USER@htcf.wustl.edu ----------------------------------------------------------------- Then in the desktop browser, follow the http://127.0.0.1..... address shown at the bottom of the jupyter lab command \" # Launch jupyter lab jupyter lab --no-browser --port=$port --ip=$HOSTNAME","title":"Jupyter"},{"location":"using/queue/","text":"Queuing System - Slurm The HTCF utlizes the Simple Linux Utility for Resource Management (Slurm). Slurm documentation can be found at http://slurm.schedmd.com/documentation.html . Job Submission There are two type of Slurm jobs, batch and interactive. Batch Jobs The steps needed to submit batch jobs are: Create a \"job script\". This is the file that actually does the work of the job. Create a \"sbatch script\". This file sets the Slurm parameters and prepares the environment for the job script. Launch the job using the \"sbatch\" command salloc - Obtains a job allocation. --cpu-per-task - Number of CPUs required per task --dependency=<state:jobid> --job-name=<name> --mem=<MB> Memory required per node. --mem-per-cpu=<MB> Memory required per allocated CPU. sbatch - Submits batch scripts for execution. srun - Obtains job allocation and executes an application. Partitions Partition Max Memory Duration Max CPUs in Queue general 250GB no limit 3004 interactive 250GB 8 hours 3004 Job Management squeue To view your job status in the queue scancel Users can use scancel command to cancel their jobs or job arrays. You may see job states of CA or CG during this processes. scancel JOBID scancel -u $USER Job Accounting sacct is the command to view all previously run job information. You can get a list of viewable fields by running the command sacct -e To view a past jobs maximum used memory and duration sacct -j JOBID --format=JobID,JobName,MaxRSS,Elapsed Scontrol can be used to view detailed information about your running job including the job script that was submitted. Please send the output of this command if your currently running job is having issues. ~$ scontrol show jobid -dd 846115 JobId=846115 JobName=sleep.sh UserId=ericmartin(1002) GroupId=ericmartin(1002) Priority=3070 Nice=0 Account=htcfadmin QOS=normal JobState=RUNNING Reason=None Dependency=(null) Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 DerivedExitCode=0:0 RunTime=00:00:09 TimeLimit=UNLIMITED TimeMin=N/A SubmitTime=2016-03-09T09:47:08 EligibleTime=2016-03-09T09:47:08 StartTime=2016-03-09T09:47:09 EndTime=Unknown PreemptTime=None SuspendTime=None SecsPreSuspend=0 Partition=general AllocNode:Sid=n082:21380 ReqNodeList=(null) ExcNodeList=(null) NodeList=n082 BatchHost=n082 NumNodes=1 NumCPUs=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:* Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* Nodes=n082 CPU_IDs=3-4 Mem=2000 MinCPUsNode=1 MinMemoryCPU=1000M MinTmpDiskNode=0 Features=(null) Gres=(null) Reservation=(null) Shared=OK Contiguous=0 Licenses=(null) Network=(null) Command=/scratch/htcfadmin/eric/sleep.sh WorkDir=/scratch/htcfadmin/eric StdErr=/scratch/htcfadmin/eric/slurm-846115.out StdIn=/dev/null StdOut=/scratch/htcfadmin/eric/slurm-846115.out BatchScript= #!/bin/bash #SBATCH -n 2 #SBATCH -N 1 module load bowtie2 sleep 100 More information on usage is available at http://slurm.schedmd.com/sacct.html . More information available here: * http://slurm.schedmd.com/overview.html * http://slurm.schedmd.com/tutorials.html * http://slurm.schedmd.com/faq.html","title":"Queue"},{"location":"using/queue/#queuing-system-slurm","text":"The HTCF utlizes the Simple Linux Utility for Resource Management (Slurm). Slurm documentation can be found at http://slurm.schedmd.com/documentation.html .","title":"Queuing System - Slurm"},{"location":"using/queue/#job-submission","text":"There are two type of Slurm jobs, batch and interactive.","title":"Job Submission"},{"location":"using/queue/#batch-jobs","text":"The steps needed to submit batch jobs are: Create a \"job script\". This is the file that actually does the work of the job. Create a \"sbatch script\". This file sets the Slurm parameters and prepares the environment for the job script. Launch the job using the \"sbatch\" command salloc - Obtains a job allocation. --cpu-per-task - Number of CPUs required per task --dependency=<state:jobid> --job-name=<name> --mem=<MB> Memory required per node. --mem-per-cpu=<MB> Memory required per allocated CPU. sbatch - Submits batch scripts for execution. srun - Obtains job allocation and executes an application.","title":"Batch Jobs"},{"location":"using/queue/#partitions","text":"Partition Max Memory Duration Max CPUs in Queue general 250GB no limit 3004 interactive 250GB 8 hours 3004","title":"Partitions"},{"location":"using/queue/#job-management","text":"","title":"Job Management"},{"location":"using/queue/#squeue","text":"To view your job status in the queue","title":"squeue"},{"location":"using/queue/#scancel","text":"Users can use scancel command to cancel their jobs or job arrays. You may see job states of CA or CG during this processes. scancel JOBID scancel -u $USER","title":"scancel"},{"location":"using/queue/#job-accounting","text":"sacct is the command to view all previously run job information. You can get a list of viewable fields by running the command sacct -e To view a past jobs maximum used memory and duration sacct -j JOBID --format=JobID,JobName,MaxRSS,Elapsed Scontrol can be used to view detailed information about your running job including the job script that was submitted. Please send the output of this command if your currently running job is having issues. ~$ scontrol show jobid -dd 846115 JobId=846115 JobName=sleep.sh UserId=ericmartin(1002) GroupId=ericmartin(1002) Priority=3070 Nice=0 Account=htcfadmin QOS=normal JobState=RUNNING Reason=None Dependency=(null) Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 DerivedExitCode=0:0 RunTime=00:00:09 TimeLimit=UNLIMITED TimeMin=N/A SubmitTime=2016-03-09T09:47:08 EligibleTime=2016-03-09T09:47:08 StartTime=2016-03-09T09:47:09 EndTime=Unknown PreemptTime=None SuspendTime=None SecsPreSuspend=0 Partition=general AllocNode:Sid=n082:21380 ReqNodeList=(null) ExcNodeList=(null) NodeList=n082 BatchHost=n082 NumNodes=1 NumCPUs=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:* Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* Nodes=n082 CPU_IDs=3-4 Mem=2000 MinCPUsNode=1 MinMemoryCPU=1000M MinTmpDiskNode=0 Features=(null) Gres=(null) Reservation=(null) Shared=OK Contiguous=0 Licenses=(null) Network=(null) Command=/scratch/htcfadmin/eric/sleep.sh WorkDir=/scratch/htcfadmin/eric StdErr=/scratch/htcfadmin/eric/slurm-846115.out StdIn=/dev/null StdOut=/scratch/htcfadmin/eric/slurm-846115.out BatchScript= #!/bin/bash #SBATCH -n 2 #SBATCH -N 1 module load bowtie2 sleep 100 More information on usage is available at http://slurm.schedmd.com/sacct.html . More information available here: * http://slurm.schedmd.com/overview.html * http://slurm.schedmd.com/tutorials.html * http://slurm.schedmd.com/faq.html","title":"Job Accounting"}]}