{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"High Throughput Computing Facility @ CGS_SB The Center for Genome Sciences and Systems Biology is the proud home of The High Throughput Computing Facility, a Washington University recharge center . The HTCF provides high-throughput computational resources for researchers within the CGS_SB. An HTCF description for grant writing purposes The CGSSB provides a computational cluster for high-throughput bioinformatics. The cluster consists of over 2750 processors and over 20TB of RAM. It supports the Center's Illumina sequencing platforms and real time sequencing analysis. Long term data storage is handled by our 10/40 GbE connected storage arrays. These arrays are currently over 2 petabytes in size. All user data is backed up and stored daily, weekly and monthly. A disaster recovery copy of select data is stored offsite. The HTCF also includes a 600TB high-speed distributed file system that is capable of throughput up to 19GB/s. This, coupled with its 100Gb network backbone, delivering 10Gb of bandwidth to cluster nodes, the HTCF can provide exceptionally high-speed transfer of large amounts of data.","title":"High Throughput Computing Facility @ CGS_SB"},{"location":"#high-throughput-computing-facility-cgs_sb","text":"The Center for Genome Sciences and Systems Biology is the proud home of The High Throughput Computing Facility, a Washington University recharge center . The HTCF provides high-throughput computational resources for researchers within the CGS_SB. An HTCF description for grant writing purposes The CGSSB provides a computational cluster for high-throughput bioinformatics. The cluster consists of over 2750 processors and over 20TB of RAM. It supports the Center's Illumina sequencing platforms and real time sequencing analysis. Long term data storage is handled by our 10/40 GbE connected storage arrays. These arrays are currently over 2 petabytes in size. All user data is backed up and stored daily, weekly and monthly. A disaster recovery copy of select data is stored offsite. The HTCF also includes a 600TB high-speed distributed file system that is capable of throughput up to 19GB/s. This, coupled with its 100Gb network backbone, delivering 10Gb of bandwidth to cluster nodes, the HTCF can provide exceptionally high-speed transfer of large amounts of data.","title":"High Throughput Computing Facility @ CGS_SB"},{"location":"getstarted/","text":"Your HTCF Account Account Creation To request a user account on the HTCF, please send an email with your request, along with your WUSTLKey username and your department ID (for billing purposes). Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account. Logging In Your WUSTLKey credentials are used for authentication ( http://wustlkey.wustl.edu/ ) The login server, login.htcf.wustl.edu is accessible via ssh. Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account. Using your account Before using the HTCF, it's important to familiarize yourself with: The HTCF Storage including home directories , long term storage , and reference storage Using software on the HTCF The Slurm Workload Manager queue and running jobs Data & Data Storage Home Directories Each HTCF user account has 20GB home directory. This directory can be used to store scripts, development tools, etc. Home directories are located in \"/home/WUSTL_KEY_ID\" and are available on all nodes. They are kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. Long Term Storage LTS is used to store raw and \"finished\" project data. The LTS directories are not available on the cluster nodes. Long term storage is lab project space, available in terabyte increments. It is kept on fault-tolerant storage with snapshops. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. High Throughput Storage HTCF high throughput storage is a large, distrubuted file system able to handle more than 6GB/second of total throughput. The HTS is scratch space and is not backed up. High thoughput storage is temporary. We absolutely cannot recover anything in /scratch once removed. *Data stored in /scratch is subject to the Scratch Data Cleaning Policy . More information is available on our Data & Data Storage Page Software List the software available on HTCF module avail To load the software: module load bowtie2 To request additional software modules, please contact us. More information is available on our Software Page GUI Software Note As the HTCF is primarily a batch queuing system for high-throughput processing of data, use of GUI applications are not directly supported. While use of GUI applications is possible using X forwarding, this can sometimes require significant desktop preparation and configuration which is beyond the scope of support. Workflow Jobs typically follow a generic workflow. A - Preprocessed Raw Data Enters LTS B - Raw Data is copied to scratch for processing C - Post processed data is copied to LTS D - Intermediate data generated in Step B is removed Partitions Partition Max Memory Duration Max CPUs in Queue debug 250GB no limit 3004 interactive 250GB 8 hours 3004 Jobs Interactive Interactive sessions are for running interactive scripts, vizualization, any tasks that are too computational intensive to run on the login node not submitted via sbatch. The defaults are: 1 CPU core, 1 GB RAM, and a time limit of 8 hours. Note The HTCF is primarily a batch queuing system. Interactive jobs are meant to function as daily workspaces. Because interactive jobs are by their nature, inefficient, they are not meant to be running continuously for more than 1 day. When using interactive tools such as rstudio or jupyter, please make sure the jobs are using the \"interactive\" queue (using sbatch/srun parameters -J interactive -p interactive ) Jobs using interactive tools that are not in the interactive queue will be subject to cancellation in order to free up resources for batch jobs. Tools such as Rscript can be used to run R programs in a batch fashion. It appears that jupyter notebooks can also be run in a batch fashion . Thanks for helping to ensure fairness for all folks on the HTCF. You can create an interactive session by running: ~$ srun --mem=<MBs> --cpus-per-task=<num> -J interactive -p interactive --pty /bin/bash -l Batch Job Submission Determine resources Create Job File Create sbatch file with required resources Submit Monitor Sbatch Examples Create a job script (myjob.sbatch): #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem=1G ml program program /scratch/lab/files/ABC.fasta /scratch/lab/files/ABC.out Submit the sbatch script. sbatch myjob.sbatch View the job in the queue user@htcf:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 106 debug example example R 0:13 1 n067 GPUs The HTCF currently has a small number of NVIDIA Tesla V100 GPUs. A GPU is accessible using the following slurm parameters: #SBATCH -p gpu #SBATCH --gres=gpu","title":"Getstarted"},{"location":"getstarted/#your-htcf-account","text":"","title":"Your HTCF Account"},{"location":"getstarted/#account-creation","text":"To request a user account on the HTCF, please send an email with your request, along with your WUSTLKey username and your department ID (for billing purposes). Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account.","title":"Account Creation"},{"location":"getstarted/#logging-in","text":"Your WUSTLKey credentials are used for authentication ( http://wustlkey.wustl.edu/ ) The login server, login.htcf.wustl.edu is accessible via ssh. Note As stated in the WUSTL and HTCF Policies , accounts and passwords cannot be shared. All users must have their own account.","title":"Logging In"},{"location":"getstarted/#using-your-account","text":"Before using the HTCF, it's important to familiarize yourself with: The HTCF Storage including home directories , long term storage , and reference storage Using software on the HTCF The Slurm Workload Manager queue and running jobs","title":"Using your account"},{"location":"getstarted/#data-data-storage","text":"","title":"Data &amp; Data Storage"},{"location":"getstarted/#home-directories","text":"Each HTCF user account has 20GB home directory. This directory can be used to store scripts, development tools, etc. Home directories are located in \"/home/WUSTL_KEY_ID\" and are available on all nodes. They are kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes.","title":"Home Directories"},{"location":"getstarted/#long-term-storage","text":"LTS is used to store raw and \"finished\" project data. The LTS directories are not available on the cluster nodes. Long term storage is lab project space, available in terabyte increments. It is kept on fault-tolerant storage with snapshops. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes.","title":"Long Term Storage"},{"location":"getstarted/#high-throughput-storage","text":"HTCF high throughput storage is a large, distrubuted file system able to handle more than 6GB/second of total throughput. The HTS is scratch space and is not backed up. High thoughput storage is temporary. We absolutely cannot recover anything in /scratch once removed. *Data stored in /scratch is subject to the Scratch Data Cleaning Policy . More information is available on our Data & Data Storage Page","title":"High Throughput Storage"},{"location":"getstarted/#software","text":"List the software available on HTCF module avail To load the software: module load bowtie2 To request additional software modules, please contact us. More information is available on our Software Page","title":"Software"},{"location":"getstarted/#gui-software","text":"Note As the HTCF is primarily a batch queuing system for high-throughput processing of data, use of GUI applications are not directly supported. While use of GUI applications is possible using X forwarding, this can sometimes require significant desktop preparation and configuration which is beyond the scope of support.","title":"GUI Software"},{"location":"getstarted/#workflow","text":"Jobs typically follow a generic workflow. A - Preprocessed Raw Data Enters LTS B - Raw Data is copied to scratch for processing C - Post processed data is copied to LTS D - Intermediate data generated in Step B is removed","title":"Workflow"},{"location":"getstarted/#partitions","text":"Partition Max Memory Duration Max CPUs in Queue debug 250GB no limit 3004 interactive 250GB 8 hours 3004","title":"Partitions"},{"location":"getstarted/#jobs","text":"","title":"Jobs"},{"location":"getstarted/#interactive","text":"Interactive sessions are for running interactive scripts, vizualization, any tasks that are too computational intensive to run on the login node not submitted via sbatch. The defaults are: 1 CPU core, 1 GB RAM, and a time limit of 8 hours. Note The HTCF is primarily a batch queuing system. Interactive jobs are meant to function as daily workspaces. Because interactive jobs are by their nature, inefficient, they are not meant to be running continuously for more than 1 day. When using interactive tools such as rstudio or jupyter, please make sure the jobs are using the \"interactive\" queue (using sbatch/srun parameters -J interactive -p interactive ) Jobs using interactive tools that are not in the interactive queue will be subject to cancellation in order to free up resources for batch jobs. Tools such as Rscript can be used to run R programs in a batch fashion. It appears that jupyter notebooks can also be run in a batch fashion . Thanks for helping to ensure fairness for all folks on the HTCF. You can create an interactive session by running: ~$ srun --mem=<MBs> --cpus-per-task=<num> -J interactive -p interactive --pty /bin/bash -l","title":"Interactive"},{"location":"getstarted/#batch-job-submission","text":"Determine resources Create Job File Create sbatch file with required resources Submit Monitor","title":"Batch Job Submission"},{"location":"getstarted/#sbatch-examples","text":"Create a job script (myjob.sbatch): #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem=1G ml program program /scratch/lab/files/ABC.fasta /scratch/lab/files/ABC.out Submit the sbatch script. sbatch myjob.sbatch View the job in the queue user@htcf:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 106 debug example example R 0:13 1 n067","title":"Sbatch Examples"},{"location":"getstarted/#gpus","text":"The HTCF currently has a small number of NVIDIA Tesla V100 GPUs. A GPU is accessible using the following slurm parameters: #SBATCH -p gpu #SBATCH --gres=gpu","title":"GPUs"},{"location":"globus/","text":"To register a personal HTCF endpoint with Globus. (only needs to be run once) $ globusconnectpersonal -setup Open the link presented in a web browser and follow the on screen instructions. Copy the auth code given at the end of the registration and paste it in the terminal where prompted. To start the endpoint: $ globusconnectpersonal -start -restrict-paths /location/you/want/to/access/files For example, if transferring data to/from /scratch/mylab/mydir/project $ globusconnectpersonal -start -restrict-paths /scratch/mylab/mydir/project For long running transfers, globusconnectpersonal can be run in a screen session or an sbatch job. Once the endpoint is started, transfers can begin. They can be initiated from the globus website, or the globus command line tool (module load py-globus-cli). Please see the Globus documentation to understand both of these methods.","title":"Globus"},{"location":"policies/","text":"Policies WUSTL Computer Use http://wustl.edu/policies/compolicy.html Account Usage As stated in the above WUSTL Policy: \"Do not use the password of others or access files under false identity.\" Accounts and passwords cannot be shared. All users must have their own account. Account Renewal HTCF user accounts are automatically renewed annually from the original activation date unless otherwise instructed. Account Removal Home directories of expired accounts are removed 90 days after expiration. Storage Policies Scratch Data Cleaning In order to ensure top performance of /scratch it is important to clean it regularly to remove stale data. Therefore, the following weekly automated tasks are performed on /scratch: User files on scratch that have not been modified for more than 60 days are garbage collected and placed in a \u201ctrash\u201d location. After 30 days in the trash location, user files are purged from the system. Once purged, there is no way files can be restored. Please ensure that any files you need for more than 60 days are safely copied to an LTS bucket. Garbage-collected files are stored in /scratch/trash/<date_of_collection>/. You can restore your garbage-collected files by moving them out of this directory. A list of your garbage-collected files can be found in /scratch/trash/<date_of_collection>/filelists/<username>. The HTCF is not responsible for data loss from automated scrubs. Labs are responsible for monitoring their files and transferring their data from scratch to long term storage. Data Limits Each member of the HTCF belongs to at least two Unix groups. The primary group is your personal group, having the same name as your HTCF username. The secondary group is the laboratory or similar entity that you are primarily associated with. Policy: Scratch user data limits Size Limit - 2TB Inode Limit (Number of files) - 2,000,000 Example Username: johnsmith To determine your personal scratch usage: $ beegfs-ctl --getquota --uid johnsmith Login Node Policy The HTCF login node is to be used for job composition, software installation, and staging of job data. Any computational processes found running longer than 30 minutes can be terminated. General Availability Effort will be made to keep our resources available. Although the support personnel will do their best to keep the facility running at all times, we cannot guarantee to promptly resolve problems outside office hours, during weekends, and public holidays. Nevertheless, please notify us of whenever they arise. General Maintenance Occasionally, it is necessary as part of maintaining a reliable service to update system software and replace faulty hardware. Sometimes it will be possible to perform these tasks transparently by means of queue reconfiguration in a way that will not disrupt running jobs or interactive use, or significantly inconvenience users. Some tasks however, particularly those affecting storage or login nodes, may require temporary interruption of service. Running Jobs Jobs that improperly perform excessive I/O, or utilize unreserved CPU time will be terminated. Please be accurate when requesting memory, not requesting enough memory will result in your process crashing, requesting too much memory will prevent other users from running jobs.","title":"Policies"},{"location":"policies/#policies","text":"","title":"Policies"},{"location":"policies/#wustl-computer-use","text":"http://wustl.edu/policies/compolicy.html","title":"WUSTL Computer Use"},{"location":"policies/#account-usage","text":"As stated in the above WUSTL Policy: \"Do not use the password of others or access files under false identity.\" Accounts and passwords cannot be shared. All users must have their own account.","title":"Account Usage"},{"location":"policies/#account-renewal","text":"HTCF user accounts are automatically renewed annually from the original activation date unless otherwise instructed.","title":"Account Renewal"},{"location":"policies/#account-removal","text":"Home directories of expired accounts are removed 90 days after expiration.","title":"Account Removal"},{"location":"policies/#storage-policies","text":"","title":"Storage Policies"},{"location":"policies/#scratch-data-cleaning","text":"In order to ensure top performance of /scratch it is important to clean it regularly to remove stale data. Therefore, the following weekly automated tasks are performed on /scratch: User files on scratch that have not been modified for more than 60 days are garbage collected and placed in a \u201ctrash\u201d location. After 30 days in the trash location, user files are purged from the system. Once purged, there is no way files can be restored. Please ensure that any files you need for more than 60 days are safely copied to an LTS bucket. Garbage-collected files are stored in /scratch/trash/<date_of_collection>/. You can restore your garbage-collected files by moving them out of this directory. A list of your garbage-collected files can be found in /scratch/trash/<date_of_collection>/filelists/<username>. The HTCF is not responsible for data loss from automated scrubs. Labs are responsible for monitoring their files and transferring their data from scratch to long term storage.","title":"Scratch Data Cleaning"},{"location":"policies/#data-limits","text":"Each member of the HTCF belongs to at least two Unix groups. The primary group is your personal group, having the same name as your HTCF username. The secondary group is the laboratory or similar entity that you are primarily associated with. Policy: Scratch user data limits Size Limit - 2TB Inode Limit (Number of files) - 2,000,000 Example Username: johnsmith To determine your personal scratch usage: $ beegfs-ctl --getquota --uid johnsmith","title":"Data Limits"},{"location":"policies/#login-node-policy","text":"The HTCF login node is to be used for job composition, software installation, and staging of job data. Any computational processes found running longer than 30 minutes can be terminated.","title":"Login Node Policy"},{"location":"policies/#general-availability","text":"Effort will be made to keep our resources available. Although the support personnel will do their best to keep the facility running at all times, we cannot guarantee to promptly resolve problems outside office hours, during weekends, and public holidays. Nevertheless, please notify us of whenever they arise.","title":"General Availability"},{"location":"policies/#general-maintenance","text":"Occasionally, it is necessary as part of maintaining a reliable service to update system software and replace faulty hardware. Sometimes it will be possible to perform these tasks transparently by means of queue reconfiguration in a way that will not disrupt running jobs or interactive use, or significantly inconvenience users. Some tasks however, particularly those affecting storage or login nodes, may require temporary interruption of service.","title":"General Maintenance"},{"location":"policies/#running-jobs","text":"Jobs that improperly perform excessive I/O, or utilize unreserved CPU time will be terminated. Please be accurate when requesting memory, not requesting enough memory will result in your process crashing, requesting too much memory will prevent other users from running jobs.","title":"Running Jobs"},{"location":"prerequisites/","text":"A firm grasp of the following concepts and technologies are expected for users of the HTCF: Bash Shell https://www.gnu.org/software/bash/manual/html_node/index.html Bash Environment Variables https://linuxhint.com/bash-environment-variables/ Python Virtual Environments https://docs.python.org/3/tutorial/venv.html https://docs.python.org/3/library/venv.html, https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/","title":"Prerequisites"},{"location":"prerequisites/#bash-shell","text":"https://www.gnu.org/software/bash/manual/html_node/index.html","title":"Bash Shell"},{"location":"prerequisites/#bash-environment-variables","text":"https://linuxhint.com/bash-environment-variables/","title":"Bash Environment Variables"},{"location":"prerequisites/#python-virtual-environments","text":"https://docs.python.org/3/tutorial/venv.html https://docs.python.org/3/library/venv.html, https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/","title":"Python Virtual Environments"},{"location":"runningjobs/","text":"Many times you might have a command that you\u2019d like to run on a lot of samples. To run these sequentially, you might have a script like this: #!/bin/bash module load spades spades.py --careful --pe1-1 samp1-r1.fastq --pe1-2 samp1-r2.fastq -o assembly_1 spades.py --careful --pe1-1 samp2-r1.fastq --pe1-2 samp2-r2.fastq -o assembly_2 \u2026 spades.py --careful --pe1-1 samp2000-r1.fastq --pe1-2 samp2000-r2.fastq -o assembly_2000 By submitting this on the HTCF as an \u201carray job\u201d, you have the potential of running each of these commands in parallel (all at the same time) rather than sequentially (one at a time). The best case scenario could be that all 2000 commands finish in the time it takes 1 command to run! More information regarding SLURM job arrays is available at http://slurm.schedmd.com/job_array.html . Run a command or set of commands on file names that are sequentially numbered Input File 1 Input File 2 sample1-r1.fastq sample1-r2.fastq sample2-r1.fastq sample2-r2.fastq \u2026 \u2026 sample2000-r1.fastq sample2000-r2.fastq Step 1: Create an sbatch file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = ${ SLURM_ARRAY_TASK_ID } # Number between 1 and 2000 spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID } Run a command or set of commands on file names that aren\u2019t numbered sequentially Input File 1 Input File 2 sampleAAA-r1.fastq sampleAAA-r2.fastq sampleAAB-r1.fastq sampleAAB-r2.fastq \u2026 \u2026 SampleZZZ-r1.fastq SampleZZZ-r2.fastq Step 1: Create a \u201clookup\u201d file, lookup.txt AAA AAB \u2026 ZZZ Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 2: Create an sbatch file, grabbing the \u201cID\u201d in line $SLURM_ARRAY_TASK_ID from the lookup file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = $( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID } Run a command or set of commands using a complex lookup file Step 1: Create a \u201clookup\u201d file, lookup.txt 5 sampleAAA_R1 0.0001 5000 5 sampleAAA_R2 0.0001 5000 5 sampleBBB_R1 0.0005 1000 5 sampleBBB_R2 0.0005 1000 Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 3: Create an sbatch file, each job is created using the template with information from the lookup file. #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load seqtk read part1 part2 part3 part4 < < ( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) seqtk sample -s ${ part1 } /path/to/file. ${ part2 } .fastq ${ part3 } > /path/to/file. ${ part2 } _ ${ part4 } .fastq","title":"Runningjobs"},{"location":"runningjobs/#run-a-command-or-set-of-commands-on-file-names-that-are-sequentially-numbered","text":"Input File 1 Input File 2 sample1-r1.fastq sample1-r2.fastq sample2-r1.fastq sample2-r2.fastq \u2026 \u2026 sample2000-r1.fastq sample2000-r2.fastq Step 1: Create an sbatch file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = ${ SLURM_ARRAY_TASK_ID } # Number between 1 and 2000 spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID }","title":"Run a command or set of commands on file names that are sequentially numbered"},{"location":"runningjobs/#run-a-command-or-set-of-commands-on-file-names-that-arent-numbered-sequentially","text":"Input File 1 Input File 2 sampleAAA-r1.fastq sampleAAA-r2.fastq sampleAAB-r1.fastq sampleAAB-r2.fastq \u2026 \u2026 SampleZZZ-r1.fastq SampleZZZ-r2.fastq Step 1: Create a \u201clookup\u201d file, lookup.txt AAA AAB \u2026 ZZZ Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 2: Create an sbatch file, grabbing the \u201cID\u201d in line $SLURM_ARRAY_TASK_ID from the lookup file #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load spades ID = $( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) spades.py --careful --pe1-1 samp ${ ID } -r1.fastq --pe1-2 samp ${ ID } -r2.fastq -o assembly_ ${ ID }","title":"Run a command or set of commands on file names that aren\u2019t numbered sequentially"},{"location":"runningjobs/#run-a-command-or-set-of-commands-using-a-complex-lookup-file","text":"Step 1: Create a \u201clookup\u201d file, lookup.txt 5 sampleAAA_R1 0.0001 5000 5 sampleAAA_R2 0.0001 5000 5 sampleBBB_R1 0.0005 1000 5 sampleBBB_R2 0.0005 1000 Step 2: Find the number of lines in lookup file. This will be the number of tasks in your array job. $ wc -l lookup.txt 2000 lookup.txt Step 3: Create an sbatch file, each job is created using the template with information from the lookup file. #!/bin/bash #SBATCH --array=1-2000%20 # This will create 2000 tasks numbered 1-2000 and allow 20 concurrent jobs to run module load seqtk read part1 part2 part3 part4 < < ( sed -n ${ SLURM_ARRAY_TASK_ID } p lookup.txt ) seqtk sample -s ${ part1 } /path/to/file. ${ part2 } .fastq ${ part3 } > /path/to/file. ${ part2 } _ ${ part4 } .fastq","title":"Run a command or set of commands using a complex lookup file"},{"location":"software/","text":"Software on the HTCF Software-wise, the HTCF starts as a blank slate for each lab. However, this doesn't mean labs can't get up and running in a matter of minutes. Each lab has its own dedicated space to install and manage software. This reference space is located in /ref/<lab>/software . Self-service Installation Software building and installation on the HTCF is primarily self-service. Labs are free to use their /ref software directory to install software using whatever means is most comfortable. At the lab level, use of Spack to install common software is encouraged . Virtual environments can also be used if the software is well suited. Spack Spack is a package management tool designed to support multiple versions and configurations of software on a wide variety of platforms and environments. Note Installation of software on the login node is unsupported. Please build/install software from an interactive job. Tutorial See the official spack tutorial Spack Initialization To create a lab instance of the spack package manager: Download and untar a spack release into /ref/<lab_name>/software Rename (or make a symlink from) /ref/<lab_name>/software/spack-VERSION to /ref/<lab_name>/software/spack . Logout and log back in. This will ensure the spack command is available in the PATH. Install Software Using Spack see the official spack documentation Using software built by spack (ie. preparing the environment) Once spack has built software, the bash shell needs to have the proper environment variables set to access the software. This is accomplished using the spack load command. Using spack to set the environment Spack packages can be \"loaded\" similar to the way modules are loaded. Given a spec , a spack command can be used to generate the appropriate environment variables to \"load\" spack-installed software. To view the environment variables that would be set: $ spack load --sh <spec> To actually set the environment varibles: $ eval $( spack load --sh <spec> ) This command is similar in function to the older module load <spec> commands. See the official spack documentation for more information on specs. These commands can be placed in an sbatch file to be used in a job. #!/bin/bash eval $( spack load --sh <spec> ) Example: install biom-format $ spack list biom ==> 7 packages. microbiomeutil py-biomine r-biomart r-biomformat py-biom-format r-biom-utils r-biomartr $ spack versions py-biom-format ==> Safe versions (already checksummed): 2.1.10 2.1.9 2.1.7 2.1.6 ... $ spack install py-biom-format@2.1.10 ... $ eval $( spack load --sh py-biom-format@2.1.10 ) What if the software I want is not available through Spack When needed software is not readily accessible via Spack, there are a few options. Follow the installation instructions from the software creator. Sometimes, this can be very quick and straightforward. Sometimes, this can be very painful. Sometimes, it can be a good idea to pass judgement on the quality of software based on the quality of the installation process and documentation. Create a custom spack package Spack can be a wonderful tool for creating and maintaining software. Plenty of documentation is provided for creating and maintaining custom packages , though a firm understanding of python is needed. Creating The best explanation of creating and using python virtual environments can be found in the official documentation R considerations Manual Installation Sometimes it's just easier to follow the installation steps provided by software creator. If the software depends on other software, it might be that the dependency software could be installed via spack . For example, if a piece of software is not available via Spack, but requires samtools to be installed: # Install samtools via spack $ spack install samtools # load samtools before installation $ eval $( spack load --sh samtools ) (proceed with the manual installation) Remember The best place to install software is your reference storage /ref/lab/sofware . After manual software installation, it's good practice to then create a module file Manual module files Module files that are manually created go in your reference storage in /ref/<lab>/software/modules . The lmod documentation is the best place to learn about creating module files.","title":"Software"},{"location":"software/#software-on-the-htcf","text":"Software-wise, the HTCF starts as a blank slate for each lab. However, this doesn't mean labs can't get up and running in a matter of minutes. Each lab has its own dedicated space to install and manage software. This reference space is located in /ref/<lab>/software .","title":"Software on the HTCF"},{"location":"software/#self-service-installation","text":"Software building and installation on the HTCF is primarily self-service. Labs are free to use their /ref software directory to install software using whatever means is most comfortable. At the lab level, use of Spack to install common software is encouraged . Virtual environments can also be used if the software is well suited.","title":"Self-service Installation"},{"location":"software/#spack","text":"Spack is a package management tool designed to support multiple versions and configurations of software on a wide variety of platforms and environments. Note Installation of software on the login node is unsupported. Please build/install software from an interactive job.","title":"Spack"},{"location":"software/#tutorial","text":"See the official spack tutorial","title":"Tutorial"},{"location":"software/#spack-initialization","text":"To create a lab instance of the spack package manager: Download and untar a spack release into /ref/<lab_name>/software Rename (or make a symlink from) /ref/<lab_name>/software/spack-VERSION to /ref/<lab_name>/software/spack . Logout and log back in. This will ensure the spack command is available in the PATH.","title":"Spack Initialization"},{"location":"software/#install-software-using-spack","text":"see the official spack documentation","title":"Install Software Using Spack"},{"location":"software/#using-software-built-by-spack-ie-preparing-the-environment","text":"Once spack has built software, the bash shell needs to have the proper environment variables set to access the software. This is accomplished using the spack load command.","title":"Using software built by spack (ie. preparing the environment)"},{"location":"software/#using-spack-to-set-the-environment","text":"Spack packages can be \"loaded\" similar to the way modules are loaded. Given a spec , a spack command can be used to generate the appropriate environment variables to \"load\" spack-installed software. To view the environment variables that would be set: $ spack load --sh <spec> To actually set the environment varibles: $ eval $( spack load --sh <spec> ) This command is similar in function to the older module load <spec> commands. See the official spack documentation for more information on specs. These commands can be placed in an sbatch file to be used in a job. #!/bin/bash eval $( spack load --sh <spec> )","title":"Using spack to set the environment"},{"location":"software/#example-install-biom-format","text":"$ spack list biom ==> 7 packages. microbiomeutil py-biomine r-biomart r-biomformat py-biom-format r-biom-utils r-biomartr $ spack versions py-biom-format ==> Safe versions (already checksummed): 2.1.10 2.1.9 2.1.7 2.1.6 ... $ spack install py-biom-format@2.1.10 ... $ eval $( spack load --sh py-biom-format@2.1.10 )","title":"Example: install biom-format"},{"location":"software/#what-if-the-software-i-want-is-not-available-through-spack","text":"When needed software is not readily accessible via Spack, there are a few options. Follow the installation instructions from the software creator. Sometimes, this can be very quick and straightforward. Sometimes, this can be very painful. Sometimes, it can be a good idea to pass judgement on the quality of software based on the quality of the installation process and documentation. Create a custom spack package Spack can be a wonderful tool for creating and maintaining software. Plenty of documentation is provided for creating and maintaining custom packages , though a firm understanding of python is needed.","title":"What if the software I want is not available through Spack"},{"location":"software/#creating","text":"The best explanation of creating and using python virtual environments can be found in the official documentation","title":"Creating"},{"location":"software/#r-considerations","text":"","title":"R considerations"},{"location":"software/#manual-installation","text":"Sometimes it's just easier to follow the installation steps provided by software creator. If the software depends on other software, it might be that the dependency software could be installed via spack . For example, if a piece of software is not available via Spack, but requires samtools to be installed: # Install samtools via spack $ spack install samtools # load samtools before installation $ eval $( spack load --sh samtools ) (proceed with the manual installation) Remember The best place to install software is your reference storage /ref/lab/sofware . After manual software installation, it's good practice to then create a module file","title":"Manual Installation"},{"location":"software/#manual-module-files","text":"Module files that are manually created go in your reference storage in /ref/<lab>/software/modules . The lmod documentation is the best place to learn about creating module files.","title":"Manual module files"},{"location":"software_old/","text":"Modules Lmod is a Lua based module system that easily handles the MODULEPATH Hierarchical problem. Environment Modules provide a convenient way to dynamically change the users' environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found. Software is handled using lmod . There should be minimal need to modify your .bashrc or .profile unless you're installing software locally to test. Basics Lmod is managed using the command module , using this command without options will show you a list of all available subcommands. ~$ module Modules based on Lua: Version 6.6 2016-10-13 13:28 -05:00 by Robert McLay mclay@tacc.utexas.edu module [options] sub-command [args ...] Help sub-commands: ------------------ help prints this message help module [...] print help message from module(s) A list of software available the command: ~$ module avail ---------------------------------------------- /opt/apps/modules ---------------------------------------------- GD/2.1.1 genometools/1.5.8 pindel/0.2.5b6 R/2.15.3 ghostscript/9.19 pmap/11-25-2010 R/3.1.2 glimmer/3.02b poretools/0.6.0 To load the latest (default) version of module: module load ncbi-blast To specifiy which version of the module you would like to use: module load ncbi-blast/2.2.30+ Be sure to specify which version of the sofware you'd like to use in your scripts to ensure consistent results, as software updates may break pipelines. Software with prerequisites are loaded dynamically, for example: ~$ module load prokka ~$ ml Currently Loaded Modules: 1) aragorn/1.2.36 3) prodigal/2.6.2 5) tbl2asn/24.2 7) hmmer/3.1b1 9) prokka/1.11 2) infernal/1.1.1 4) ncbi-blast/2.2.31+ 6) bio-perl/1.6.923 8) barrnap/0.6 GUI Software As the HTCF is primarily a batch queuing system for high-throughput processing of large amounts of data, GUI application are not directly supported by the HTCF. GUI application installation and setup on the HTCF are left to the end user.","title":"Software old"},{"location":"software_old/#modules","text":"Lmod is a Lua based module system that easily handles the MODULEPATH Hierarchical problem. Environment Modules provide a convenient way to dynamically change the users' environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found. Software is handled using lmod . There should be minimal need to modify your .bashrc or .profile unless you're installing software locally to test.","title":"Modules"},{"location":"software_old/#basics","text":"Lmod is managed using the command module , using this command without options will show you a list of all available subcommands. ~$ module Modules based on Lua: Version 6.6 2016-10-13 13:28 -05:00 by Robert McLay mclay@tacc.utexas.edu module [options] sub-command [args ...] Help sub-commands: ------------------ help prints this message help module [...] print help message from module(s) A list of software available the command: ~$ module avail ---------------------------------------------- /opt/apps/modules ---------------------------------------------- GD/2.1.1 genometools/1.5.8 pindel/0.2.5b6 R/2.15.3 ghostscript/9.19 pmap/11-25-2010 R/3.1.2 glimmer/3.02b poretools/0.6.0 To load the latest (default) version of module: module load ncbi-blast To specifiy which version of the module you would like to use: module load ncbi-blast/2.2.30+ Be sure to specify which version of the sofware you'd like to use in your scripts to ensure consistent results, as software updates may break pipelines. Software with prerequisites are loaded dynamically, for example: ~$ module load prokka ~$ ml Currently Loaded Modules: 1) aragorn/1.2.36 3) prodigal/2.6.2 5) tbl2asn/24.2 7) hmmer/3.1b1 9) prokka/1.11 2) infernal/1.1.1 4) ncbi-blast/2.2.31+ 6) bio-perl/1.6.923 8) barrnap/0.6","title":"Basics"},{"location":"software_old/#gui-software","text":"As the HTCF is primarily a batch queuing system for high-throughput processing of large amounts of data, GUI application are not directly supported by the HTCF. GUI application installation and setup on the HTCF are left to the end user.","title":"GUI Software"},{"location":"release-notes/2021-q1/","text":"Q1 - 2021 Changes The beginning of 2021 will see quite a few (pretty cool, we think) changes within the HTCF. Much effort has been made to ease the transitions that are necessary to make these changes. Please contact us with any questions or concerns. What's changing? There are essentially 4 changes (with more detail and schedule further down): OS Upgrade While the HTCF backend servers have been continually upgraded over the years, the cluster nodes and login server are long overdue for an OS upgrade. Note Many of the current software modules rely on system libraries. These libraries will be upgraded when the OS is upgraded. It it likely that some software packages will have errors due to the change in system libraries. Don't worry though, we've got a solution for that.... Self-Service Software As the HTCF user base grows, so do the software installation needs. Maintaining the software and modules for all labs can leave a web of dependency, version, and architecture conflicts that are difficult to untangle. The most sensible, sustainable, and flexible approach going forward is for HTCF support to move over to the passenger seat and leave the driving to the labs. Labs should now feel empowered to install and manage their own software (with us in the passenger seat to help navigate). Infrastructure is in place to ease in this, including an initial 1TB of space per lab for software storage called /ref. Speaking of which.... /ref A new form of LTS storage called /ref will be introduced. /ref is storage space for software and reference databases (think NCBI databases or software-provided reference sequences). Each lab will be given an initial 1TB of reference space, this space can be expanded at LTS prices. Speaking of new forms of LTS.... LTOS Object Storage LTS as been quietly available for a while, but now it's official. This is the cloud storage form of local LTS ( local meaning high speed and hosted within the HTCF) that uses the Amazon S3 API for access. It's well suited for large amounts of data that don't change much (e.g. raw sequencing runs). Unlike traditional LTS : LTOS buckets don't have a size limit They're available from each node no more login server copying back and forth! Aggregate transfer rates can be much faster than traditional /lts. Before you get too excited, there's a learning curve...because it's more like cloud storage, you won't find it in a traditional filesystem directory. So cp , mv , and rsync will not help you. With the right use case though, we're confident that the learning curve will be well worth it! OS Upgrade All nodes (including the login server) will be upgraded from Ubuntu 16.04 to Ubuntu 20.04. Warning Many of the current software modules rely on system libraries. These libraries will be upgraded when the OS is upgraded. It it likely that some software packages will have errors due to the change in system libraries. Date Change % nodes containing upgraded OS Jan 11 New slurm partition beta will be made available for trying out jobs on a small number of nodes that use the upgraded OS. To use, add: #SBATCH -p beta to your sbatch files 5% Jan 18 Login Server will be upgraded to new OS 5% Jan 18 - 25 Rolling OS upgrades of idle nodes (up to 50% of nodes) will continue. 5% - 50% Feb 1 Slurm partition general will be renamed legacy and beta will be renamed to general . 50% Feb 1 Slurm partition general will become the default interactive and batch job partition. 50% Feb 1 - 15 Rolling OS upgrades of idle nodes (up to 95% of nodes) will continue. 50% - 95% Mar 8 All nodes upgraded Self-service software and modules The system-wide software and modules in directories /opt/apps and /opt/htcf will be frozen (read-only) and use of software in these directories is deprecated. Replacing these directories will be a form of LTS call /ref, lab reference space located in /ref/<lab>/software and /ref/<lab>/modulefiles . Each lab will receive 1TB of space in /ref for storage of reference material including software and reference sequence databases. More information about /ref can be found here. Software installation will now be self-service. To aid in this, documentation has been added and infrastructue is in place for lab-managed spack installations . Thousands of packages are available for installation using spack . Help We are here to help with the migration from the deprecated modules system to your labs' new /ref software. Rest assured that the deprecated software will not be removed until migration is complete. Date Change Jan 11 Each lab given 1TB of /ref space which will be availabe on all OS-upgraded nodes . To begin testing: srun --mem=X000 -c X -p beta --pty /bin/bash -l Jan 18 /opt/apps and /opt/htcf will go read-only to encourage use of /ref Jan 25 modules loaded from /opt/apps and /opt/htcf will contain a Deprecation Warning to encourage use of /ref Mar 1 - 8 modules for software in /opt/apps and /opt/htcf will be incrementally removed Mar 8 /opt/apps and /opt/htcf directories will be decommissioned. /ref Note Keep in mind, while /ref is a form of LTS, it is not /lts (e.g. it isn't backed up) Date Change Jan 11 Each lab given 1TB of /ref space which will be availabe on all OS-upgraded nodes to begin testing: srun --mem=X000 -c X -p beta --pty /bin/bash -l Feb 1 The /scratch/ref data directory will be renamed /scratch/old-ref to encourage use of /ref/<lab>/data/ instead. Mar 8 /scratch/old-ref directory will be decommissioned. LTOS Date Change Jan 11 LTOS open for business Feb 1 Labs with traditional /lts buckets >= 15TB will be encouraged to consider LTOS for some of their bulky data (such as raw sequence data)","title":"Q1 - 2021 Changes"},{"location":"release-notes/2021-q1/#q1-2021-changes","text":"The beginning of 2021 will see quite a few (pretty cool, we think) changes within the HTCF. Much effort has been made to ease the transitions that are necessary to make these changes. Please contact us with any questions or concerns.","title":"Q1 - 2021 Changes"},{"location":"release-notes/2021-q1/#whats-changing","text":"There are essentially 4 changes (with more detail and schedule further down): OS Upgrade While the HTCF backend servers have been continually upgraded over the years, the cluster nodes and login server are long overdue for an OS upgrade. Note Many of the current software modules rely on system libraries. These libraries will be upgraded when the OS is upgraded. It it likely that some software packages will have errors due to the change in system libraries. Don't worry though, we've got a solution for that.... Self-Service Software As the HTCF user base grows, so do the software installation needs. Maintaining the software and modules for all labs can leave a web of dependency, version, and architecture conflicts that are difficult to untangle. The most sensible, sustainable, and flexible approach going forward is for HTCF support to move over to the passenger seat and leave the driving to the labs. Labs should now feel empowered to install and manage their own software (with us in the passenger seat to help navigate). Infrastructure is in place to ease in this, including an initial 1TB of space per lab for software storage called /ref. Speaking of which.... /ref A new form of LTS storage called /ref will be introduced. /ref is storage space for software and reference databases (think NCBI databases or software-provided reference sequences). Each lab will be given an initial 1TB of reference space, this space can be expanded at LTS prices. Speaking of new forms of LTS.... LTOS Object Storage LTS as been quietly available for a while, but now it's official. This is the cloud storage form of local LTS ( local meaning high speed and hosted within the HTCF) that uses the Amazon S3 API for access. It's well suited for large amounts of data that don't change much (e.g. raw sequencing runs). Unlike traditional LTS : LTOS buckets don't have a size limit They're available from each node no more login server copying back and forth! Aggregate transfer rates can be much faster than traditional /lts. Before you get too excited, there's a learning curve...because it's more like cloud storage, you won't find it in a traditional filesystem directory. So cp , mv , and rsync will not help you. With the right use case though, we're confident that the learning curve will be well worth it!","title":"What's changing?"},{"location":"release-notes/2021-q1/#os-upgrade","text":"All nodes (including the login server) will be upgraded from Ubuntu 16.04 to Ubuntu 20.04. Warning Many of the current software modules rely on system libraries. These libraries will be upgraded when the OS is upgraded. It it likely that some software packages will have errors due to the change in system libraries. Date Change % nodes containing upgraded OS Jan 11 New slurm partition beta will be made available for trying out jobs on a small number of nodes that use the upgraded OS. To use, add: #SBATCH -p beta to your sbatch files 5% Jan 18 Login Server will be upgraded to new OS 5% Jan 18 - 25 Rolling OS upgrades of idle nodes (up to 50% of nodes) will continue. 5% - 50% Feb 1 Slurm partition general will be renamed legacy and beta will be renamed to general . 50% Feb 1 Slurm partition general will become the default interactive and batch job partition. 50% Feb 1 - 15 Rolling OS upgrades of idle nodes (up to 95% of nodes) will continue. 50% - 95% Mar 8 All nodes upgraded","title":"OS Upgrade"},{"location":"release-notes/2021-q1/#self-service-software-and-modules","text":"The system-wide software and modules in directories /opt/apps and /opt/htcf will be frozen (read-only) and use of software in these directories is deprecated. Replacing these directories will be a form of LTS call /ref, lab reference space located in /ref/<lab>/software and /ref/<lab>/modulefiles . Each lab will receive 1TB of space in /ref for storage of reference material including software and reference sequence databases. More information about /ref can be found here. Software installation will now be self-service. To aid in this, documentation has been added and infrastructue is in place for lab-managed spack installations . Thousands of packages are available for installation using spack . Help We are here to help with the migration from the deprecated modules system to your labs' new /ref software. Rest assured that the deprecated software will not be removed until migration is complete. Date Change Jan 11 Each lab given 1TB of /ref space which will be availabe on all OS-upgraded nodes . To begin testing: srun --mem=X000 -c X -p beta --pty /bin/bash -l Jan 18 /opt/apps and /opt/htcf will go read-only to encourage use of /ref Jan 25 modules loaded from /opt/apps and /opt/htcf will contain a Deprecation Warning to encourage use of /ref Mar 1 - 8 modules for software in /opt/apps and /opt/htcf will be incrementally removed Mar 8 /opt/apps and /opt/htcf directories will be decommissioned.","title":"Self-service software and modules"},{"location":"release-notes/2021-q1/#ref","text":"Note Keep in mind, while /ref is a form of LTS, it is not /lts (e.g. it isn't backed up) Date Change Jan 11 Each lab given 1TB of /ref space which will be availabe on all OS-upgraded nodes to begin testing: srun --mem=X000 -c X -p beta --pty /bin/bash -l Feb 1 The /scratch/ref data directory will be renamed /scratch/old-ref to encourage use of /ref/<lab>/data/ instead. Mar 8 /scratch/old-ref directory will be decommissioned.","title":"/ref"},{"location":"release-notes/2021-q1/#ltos","text":"Date Change Jan 11 LTOS open for business Feb 1 Labs with traditional /lts buckets >= 15TB will be encouraged to consider LTOS for some of their bulky data (such as raw sequence data)","title":"LTOS"},{"location":"storage/","text":"Storage at the HTCF The HTCF provides three types of storage: HDS: Home Directory Storage (/home) Home directories are a small, fixed amount of storage per account. They are kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. Each HTCF user account has 20GB home directory. This directory can be used to store scripts, development tools, etc. Home directories are located in \"/home/ WUSTL_KEY_ID \" and are available on all nodes. Please keep your home directory access to a minimum in your jobs. By default, home directories will not be readable or writeable by other users of HTCF. If you wish to change this default, you can use the chmod command to give the appropriate level of access to your lab members or other collaborative users. You can check the amount of home directory with the du command. It may take a few moments for the command to complete. For example: ~$ du -csh $HOME 10G /home/<username> 10G total LTS: Long Term Storage (/lts) Long term storage is lab project space to store raw sequencing and completed data, the directories are not available on nodes for computational use. It is available in terabyte increments billed monthly. It is kept on fault-tolerant storage with snapshops. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. To see how much space your lab is using, use the example command below, replace lab with your labname. ~$ df -h $( grep -v '^#' /etc/fstab | awk '{ print $2 }' | grep '/lts/mylab' ) Filesystem Size Used Avail Use% Mounted on /dev/mylab/seq 1.0T 165G 860G 17% /lts/lab/seq /dev/mylab/data1 10T 6.4T 3.7T 64% /lts/lab/data1 /dev/mylab/data2 15T 14T 1.1T 93% /lts/lab/data2 HTS: High Throughput Storage (/scratch) HTCF high throughput storage is over 700TB, BeeGFS distrubuted file system able to handle more than 19GB/second of total throughput. This storage is temporary scratch space and is not backed up. We absolutely cannot recover anything in /scratch once removed. *Data stored in /scratch is subject to the Scratch Data Cleaning Policy . Jobs must utilize this space for inputs and outputs to provide the best performance possible. Running jobs that read/write from your home directory will cause slowness and login issues for all users. We will ask users to clean up older files on /scratch if it is needed to improve system performance. The best use of the HTS is to follow a workflow similar to this: Copy your starting (raw) data from LTS over to HTS. Submit jobs to the cluster that process your data, creating intermediate or final data. Copy your final data (and job files used to create that data) over to LTS. Remove all working data from HTS Results that are generated on this storage need to be promptly copied to LTS. We have a general quota of 2TB per user in /scratch to prevent the filesystem from filling up. At ~95% /scratch becomes very slow, at 100%, all operations on the cluster stop. To check the amount of space you're using in /scratch, you can use the command below as an example, replace the support username with your username. ~$ beegfs-ctl --getquota --uid support user/group || size || chunk files name | id || used | hard || used | hard --------------|------||------------|------------||---------|--------- support| 1000|| 325.48 GiB | 2.00 TiB || 0| 0 Quota Increase Requests To temporarily increase your scratch quota, please email with the following information: Reason for the increase Amount of additional space Duration additional space will be required Recommendations No important source code, scripts, libraries, executables should be kept in /scratch Do not make symlinks from your home directory to folders in /scratch Work with large files before breaking up into smaller files Publishing Files To temporarily generate a URL for a collaborator to download files, you can use the following commands. If there are any errors, please ensure the directory and file permissions on the files you wish to share allow all users read access. The URL will be valid for 90 days, if a shorter duration is required please contact us. Note Please ensure the you are the \"owner\" or a member of the \"group\" of the path. To share a directory: ~$ module load htcf ~$ serve /path/to/directory To share a specific file: ~$ module load htcf ~$ serve /path/to/file For long term hosting of publicly accessible data, please contact your department IT. Copying Files Using Rsync Using rsync to transfer to scratch and LTS is recommended. Rsync can resume failed copies, be re-run to ensure all of the data has been transferred, and will also transfer incremental changes. This will save a substantial amount of time if you need to verify that all files have been successfully copied. When using this command, please note that the absense of a trailing slash means the directory, with a trailing slash means the contents of that directory. Here are a few examples: ~$ rsync -aHv --progress /directory/to/transfer /destination/directory/location/ The above example would put the directory named \"transfer\" into the directory named \"location\" ~$ rsync -aHv --progress /directory/to/transfer/ /destination/directory/location/ The above example would put the contents of the directory named \"transfer\" into the directory named \"location\". Disk Quota Exceeded Errors If you receive disk quota exceeded messages, please check each storage location to ensure you have enough disk space available. Examples of the du , df , and beegfs-ctl commands to view utilization are provided for each storage type above.","title":"Overview"},{"location":"storage/#storage-at-the-htcf","text":"The HTCF provides three types of storage:","title":"Storage at the HTCF"},{"location":"storage/#hds-home-directory-storage-home","text":"Home directories are a small, fixed amount of storage per account. They are kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. Each HTCF user account has 20GB home directory. This directory can be used to store scripts, development tools, etc. Home directories are located in \"/home/ WUSTL_KEY_ID \" and are available on all nodes. Please keep your home directory access to a minimum in your jobs. By default, home directories will not be readable or writeable by other users of HTCF. If you wish to change this default, you can use the chmod command to give the appropriate level of access to your lab members or other collaborative users. You can check the amount of home directory with the du command. It may take a few moments for the command to complete. For example: ~$ du -csh $HOME 10G /home/<username> 10G total","title":"HDS: Home Directory Storage (/home)"},{"location":"storage/#lts-long-term-storage-lts","text":"Long term storage is lab project space to store raw sequencing and completed data, the directories are not available on nodes for computational use. It is available in terabyte increments billed monthly. It is kept on fault-tolerant storage with snapshops. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes. To see how much space your lab is using, use the example command below, replace lab with your labname. ~$ df -h $( grep -v '^#' /etc/fstab | awk '{ print $2 }' | grep '/lts/mylab' ) Filesystem Size Used Avail Use% Mounted on /dev/mylab/seq 1.0T 165G 860G 17% /lts/lab/seq /dev/mylab/data1 10T 6.4T 3.7T 64% /lts/lab/data1 /dev/mylab/data2 15T 14T 1.1T 93% /lts/lab/data2","title":"LTS: Long Term Storage (/lts)"},{"location":"storage/#hts-high-throughput-storage-scratch","text":"HTCF high throughput storage is over 700TB, BeeGFS distrubuted file system able to handle more than 19GB/second of total throughput. This storage is temporary scratch space and is not backed up. We absolutely cannot recover anything in /scratch once removed. *Data stored in /scratch is subject to the Scratch Data Cleaning Policy . Jobs must utilize this space for inputs and outputs to provide the best performance possible. Running jobs that read/write from your home directory will cause slowness and login issues for all users. We will ask users to clean up older files on /scratch if it is needed to improve system performance. The best use of the HTS is to follow a workflow similar to this: Copy your starting (raw) data from LTS over to HTS. Submit jobs to the cluster that process your data, creating intermediate or final data. Copy your final data (and job files used to create that data) over to LTS. Remove all working data from HTS Results that are generated on this storage need to be promptly copied to LTS. We have a general quota of 2TB per user in /scratch to prevent the filesystem from filling up. At ~95% /scratch becomes very slow, at 100%, all operations on the cluster stop. To check the amount of space you're using in /scratch, you can use the command below as an example, replace the support username with your username. ~$ beegfs-ctl --getquota --uid support user/group || size || chunk files name | id || used | hard || used | hard --------------|------||------------|------------||---------|--------- support| 1000|| 325.48 GiB | 2.00 TiB || 0| 0","title":"HTS: High Throughput Storage (/scratch)"},{"location":"storage/#quota-increase-requests","text":"To temporarily increase your scratch quota, please email with the following information: Reason for the increase Amount of additional space Duration additional space will be required","title":"Quota Increase Requests"},{"location":"storage/#recommendations","text":"No important source code, scripts, libraries, executables should be kept in /scratch Do not make symlinks from your home directory to folders in /scratch Work with large files before breaking up into smaller files","title":"Recommendations"},{"location":"storage/#publishing-files","text":"To temporarily generate a URL for a collaborator to download files, you can use the following commands. If there are any errors, please ensure the directory and file permissions on the files you wish to share allow all users read access. The URL will be valid for 90 days, if a shorter duration is required please contact us. Note Please ensure the you are the \"owner\" or a member of the \"group\" of the path. To share a directory: ~$ module load htcf ~$ serve /path/to/directory To share a specific file: ~$ module load htcf ~$ serve /path/to/file For long term hosting of publicly accessible data, please contact your department IT.","title":"Publishing Files"},{"location":"storage/#copying-files-using-rsync","text":"Using rsync to transfer to scratch and LTS is recommended. Rsync can resume failed copies, be re-run to ensure all of the data has been transferred, and will also transfer incremental changes. This will save a substantial amount of time if you need to verify that all files have been successfully copied. When using this command, please note that the absense of a trailing slash means the directory, with a trailing slash means the contents of that directory. Here are a few examples: ~$ rsync -aHv --progress /directory/to/transfer /destination/directory/location/ The above example would put the directory named \"transfer\" into the directory named \"location\" ~$ rsync -aHv --progress /directory/to/transfer/ /destination/directory/location/ The above example would put the contents of the directory named \"transfer\" into the directory named \"location\".","title":"Copying Files Using Rsync"},{"location":"storage/#disk-quota-exceeded-errors","text":"If you receive disk quota exceeded messages, please check each storage location to ensure you have enough disk space available. Examples of the du , df , and beegfs-ctl commands to view utilization are provided for each storage type above.","title":"Disk Quota Exceeded Errors"},{"location":"storage/home/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon! Each HTCF user account has a 20GB home directory. This directory can be used to store scripts, development tools, etc. Home directories are located in /home/WUSTLKEY_NAME \" and are available on all nodes. They are kept on fault-tolerant storage and frequent snapshops are taken to prevent accidental data loss. Copies of the latest daily snapshots are kept offsite for disaster recovery purposes.","title":"Home"},{"location":"storage/hts/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Hts"},{"location":"storage/ltos/","text":"Long Term Object Storage Long Term Object Storage (LTOS) is an architecture that manages data as objects , as opposed to traditional LTS which uses file systems and block storage. Access to LTOS is REST -based (HTTP). Although LTOS uses a subset of the Amazon s3 API , LTOS data is stored internally , not in Amazon. Unlike LTS, which is only accessible from the login server, LTOS is accessible from all HTCF nodes. Purpose LTOS can be a good alternative to LTS any time the data in question doesn't need to be heavily manipulated or modified. Good Candidates for LTOS: Raw sequence data and finished analysis data Archived or rarely referenced data such as alumni files. Data that is not often modified, once created. Not Good Candidates for LTOS: scripts in use or under development software intermediate files generated during job processing Using LTOS Software Tools CLI The two most common command line tools for accessing LTOS are s3cmd and aws-cli. s3cmd First a configuration files needs to be created Workflow Putting files in LTOS Examples Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Long Term Object Storage"},{"location":"storage/ltos/#long-term-object-storage","text":"Long Term Object Storage (LTOS) is an architecture that manages data as objects , as opposed to traditional LTS which uses file systems and block storage. Access to LTOS is REST -based (HTTP). Although LTOS uses a subset of the Amazon s3 API , LTOS data is stored internally , not in Amazon. Unlike LTS, which is only accessible from the login server, LTOS is accessible from all HTCF nodes.","title":"Long Term Object Storage"},{"location":"storage/ltos/#purpose","text":"LTOS can be a good alternative to LTS any time the data in question doesn't need to be heavily manipulated or modified. Good Candidates for LTOS: Raw sequence data and finished analysis data Archived or rarely referenced data such as alumni files. Data that is not often modified, once created. Not Good Candidates for LTOS: scripts in use or under development software intermediate files generated during job processing","title":"Purpose"},{"location":"storage/ltos/#using-ltos","text":"","title":"Using LTOS"},{"location":"storage/ltos/#software-tools","text":"","title":"Software Tools"},{"location":"storage/ltos/#cli","text":"The two most common command line tools for accessing LTOS are s3cmd and aws-cli.","title":"CLI"},{"location":"storage/ltos/#s3cmd","text":"First a configuration files needs to be created","title":"s3cmd"},{"location":"storage/ltos/#workflow","text":"","title":"Workflow"},{"location":"storage/ltos/#putting-files-in-ltos","text":"","title":"Putting files in LTOS"},{"location":"storage/ltos/#examples","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Examples"},{"location":"storage/lts/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Lts"},{"location":"storage/new_index_draft/","text":".md-typeset table:not([class]) th:nth-child(2) { text-align: center; background-color: #4473c5; } .md-typeset table:not([class]) th:nth-child(3) { background-color: #70ad46; } .md-typeset table:not([class]) th:nth-child(4) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(4), .md-typeset table:not([class]) th:nth-child(6) { color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(5) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(6) { background-color: #ed7d31; } .md-typeset table:not([class]) th:nth-child(6) :text { visibility: hidden; } .md-typeset table:not([class]) thead th { font-size: 1rem; } .md-typeset table:not([class]) thead th, .md-typeset table:not([class]) tbody tr:nth-child(1) td, .md-typeset table:not([class]) td:nth-child(1) { font-weight: bold; } .md-typeset table:not([class]) td:nth-child(2) { background-color: #dae3f4; } .md-typeset table:not([class]) td:nth-child(3) { background-color: #e2f0d9; } .md-typeset table:not([class]) td:nth-child(4) { background-color: #f4b184; } .md-typeset table:not([class]) td:nth-child(5) { background-color: #f8cbac; } .md-typeset table:not([class]) td:nth-child(6) { background-color: #fbe5d7; } .md-typeset table:not([class]) tr:nth-child(1) td:nth-child(2) { background-color: #8fabda; } .md-typeset table:not([class]) tr:nth-child(1) td:nth-child(3) { background-color: #a8d18d; } table.storage thead tr.sub th.hts { background-color: #a8d18d; } table.storage thead tr.sub th.lts { background-color: #f4b184; } table.storage thead tr.sub th.ltos { background-color: #f7ccac; } table.storage thead tr.sub th.ref { background-color: #fbe5d7; } table.storage tbody td.lts { background-color: #f4b184; } .md-sidebar { display: none; } .md-content { max-width: 100%; } Note This page is a work-in-progress. New documentation for 2022 coming soon! Home HTS LTS LTS LTS Home Directory (/home) Scratch Space (/scratch) Filesystem (/lts) Object Store (LTOS) Reference (/ref) Purpose - environment customizations and scripts - development and storage of small personal project TEMPORARY STORAGE FOR: raw/initial data needed for processing by running/pending jobs intermediate data generated by running jobs final processed data prior to moving data to safer location (LTS) after jobs complete Long Term Storage For: raw/initial data such as sequencer data finished (fully processed) data documentation describing how to reproduce fully processed data from initial data Long Term Storage For: raw/initial data such as sequencer data finished (fully processed) data documentation describing how to reproduce fully processed data from initial data Long Term Storage For: Software tools needed by jobs for the processing of data reference data such as reference genomes and NCBI databases Initial Size 20G 2TB per account - - 1TB per lab Increase Cost NA $6.73/TB/Month $6.73/TB/Month $6.73/TB/Month Size Limit 20G Temporary increase available as resources allow. Please provide # TBs needed and duration of need 10 TB per bucket No limit on # of buckets - - Access from All HTCF Nodes All HTCF Nodes Login Node Only All HTCF Nodes All HTCF Nodes Access Type Standard Filesystem Standard Filesystem Standard Filesystem / lts / / HTTP interface compatible with (but not using) Amazon S3 API Standard Filesystem /ref/ /data /ref/ /software /ref/ /modules Access Speed Slow 10+ GB/s (aggregate) Max 200 MB/s 1000+ MB/s (aggregate) 100 MB/s (aggregate) Backup Policy Onsite daily/weekly/monthly snapshots as resources allow Offsite backup daily NO BACKUPS Onsite daily/weekly/mont hly snapshots as resources allow Offsite backup daily continually synced offsite user customizable: versioning of objects schedule removal of old objects NO BACKUPS Cleaning Policy - See the scratch data cleaning policy - - -","title":"New index draft"},{"location":"storage/ref/","text":"Reference Storage (/ref) Note This page is a work-in-progress. New documentation for 2022 coming soon! Each lab or group with accounts on the HTCF is given reference space in which to store software and/or reference data . /ref is available from any node in the HTCF. While there is no automated cleaning of files in /ref , there is no guaranteed backup of data in /ref . Therefore, Any data in /ref that cannot be recreated should be copied to long term storage (LTS) for safe-keeping. Structure /ref \u251c\u2500\u2500 <lab> \u251c\u2500\u2500 data \u2514\u2500\u2500 software /data The data directory is well suited for modestly sized reference data such as NCBI blast databases . Larger datasets (> 500GB) are probably better suited for Long Term Object Storage /software","title":"Reference Storage (/ref)"},{"location":"storage/ref/#reference-storage-ref","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon! Each lab or group with accounts on the HTCF is given reference space in which to store software and/or reference data . /ref is available from any node in the HTCF. While there is no automated cleaning of files in /ref , there is no guaranteed backup of data in /ref . Therefore, Any data in /ref that cannot be recreated should be copied to long term storage (LTS) for safe-keeping.","title":"Reference Storage (/ref)"},{"location":"storage/ref/#structure","text":"/ref \u251c\u2500\u2500 <lab> \u251c\u2500\u2500 data \u2514\u2500\u2500 software","title":"Structure"},{"location":"storage/ref/#data","text":"The data directory is well suited for modestly sized reference data such as NCBI blast databases . Larger datasets (> 500GB) are probably better suited for Long Term Object Storage","title":"/data"},{"location":"storage/ref/#software","text":"","title":"/software"},{"location":"using/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon! Once your software is set up and your data is placed in the proper storage location you are ready to get down to work. Work is done on the HTCF via use of the Workload Management System (or Job Scheduler). Slurm Scheduler Overview Here HTCF Layout HERE (partitions) batch jobs and monitoring of running batch jobs interactive jobs job accounting","title":"Index"},{"location":"using/accounting/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Accounting"},{"location":"using/batch/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Batch"},{"location":"using/interactive/","text":"Note This page is a work-in-progress. New documentation for 2022 coming soon!","title":"Interactive"},{"location":"using/jupyter/","text":"#!/bin/bash #SBATCH --mem=1G #SBATCH --cpus-per-task=1 #SBATCH --time 8:00:00 #SBATCH --output jupyter-lab-%J.out . env/bin/activate unset XDG_RUNTIME_DIR port=$(shuf -i9000-9999 -n1) echo -e \" Run in a new local terminal window to create an SSH tunnel to $host ----------------------------------------------------------------- ssh -N -L $port:$host:$port $USER@htcf.wustl.edu ----------------------------------------------------------------- Then in the desktop browser, follow the http://127.0.0.1..... address shown at the bottom of the jupyter lab command \" # Launch jupyter lab jupyter lab --no-browser --port=$port --ip=$HOSTNAME","title":"Jupyter"},{"location":"using/queue/","text":"Queuing System - Slurm The HTCF utlizes the Simple Linux Utility for Resource Management (Slurm). Slurm documentation can be found at http://slurm.schedmd.com/documentation.html . Job Submission There are two type of Slurm jobs, batch and interactive. Batch Jobs The steps needed to submit batch jobs are: Create a \"job script\". This is the file that actually does the work of the job. Create a \"sbatch script\". This file sets the Slurm parameters and prepares the environment for the job script. Launch the job using the \"sbatch\" command salloc - Obtains a job allocation. --cpu-per-task - Number of CPUs required per task --dependency=<state:jobid> --job-name=<name> --mem=<MB> Memory required per node. --mem-per-cpu=<MB> Memory required per allocated CPU. sbatch - Submits batch scripts for execution. srun - Obtains job allocation and executes an application. Partitions Partition Max Memory Duration Max CPUs in Queue general 250GB no limit 3004 interactive 250GB 8 hours 3004 Job Management squeue To view your job status in the queue scancel Users can use scancel command to cancel their jobs or job arrays. You may see job states of CA or CG during this processes. scancel JOBID scancel -u $USER Job Accounting sacct is the command to view all previously run job information. You can get a list of viewable fields by running the command sacct -e To view a past jobs maximum used memory and duration sacct -j JOBID --format=JobID,JobName,MaxRSS,Elapsed Scontrol can be used to view detailed information about your running job including the job script that was submitted. Please send the output of this command if your currently running job is having issues. ~$ scontrol show jobid -dd 846115 JobId=846115 JobName=sleep.sh UserId=ericmartin(1002) GroupId=ericmartin(1002) Priority=3070 Nice=0 Account=htcfadmin QOS=normal JobState=RUNNING Reason=None Dependency=(null) Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 DerivedExitCode=0:0 RunTime=00:00:09 TimeLimit=UNLIMITED TimeMin=N/A SubmitTime=2016-03-09T09:47:08 EligibleTime=2016-03-09T09:47:08 StartTime=2016-03-09T09:47:09 EndTime=Unknown PreemptTime=None SuspendTime=None SecsPreSuspend=0 Partition=general AllocNode:Sid=n082:21380 ReqNodeList=(null) ExcNodeList=(null) NodeList=n082 BatchHost=n082 NumNodes=1 NumCPUs=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:* Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* Nodes=n082 CPU_IDs=3-4 Mem=2000 MinCPUsNode=1 MinMemoryCPU=1000M MinTmpDiskNode=0 Features=(null) Gres=(null) Reservation=(null) Shared=OK Contiguous=0 Licenses=(null) Network=(null) Command=/scratch/htcfadmin/eric/sleep.sh WorkDir=/scratch/htcfadmin/eric StdErr=/scratch/htcfadmin/eric/slurm-846115.out StdIn=/dev/null StdOut=/scratch/htcfadmin/eric/slurm-846115.out BatchScript= #!/bin/bash #SBATCH -n 2 #SBATCH -N 1 module load bowtie2 sleep 100 More information on usage is available at http://slurm.schedmd.com/sacct.html . More information available here: * http://slurm.schedmd.com/overview.html * http://slurm.schedmd.com/tutorials.html * http://slurm.schedmd.com/faq.html","title":"Queue"},{"location":"using/queue/#queuing-system-slurm","text":"The HTCF utlizes the Simple Linux Utility for Resource Management (Slurm). Slurm documentation can be found at http://slurm.schedmd.com/documentation.html .","title":"Queuing System - Slurm"},{"location":"using/queue/#job-submission","text":"There are two type of Slurm jobs, batch and interactive.","title":"Job Submission"},{"location":"using/queue/#batch-jobs","text":"The steps needed to submit batch jobs are: Create a \"job script\". This is the file that actually does the work of the job. Create a \"sbatch script\". This file sets the Slurm parameters and prepares the environment for the job script. Launch the job using the \"sbatch\" command salloc - Obtains a job allocation. --cpu-per-task - Number of CPUs required per task --dependency=<state:jobid> --job-name=<name> --mem=<MB> Memory required per node. --mem-per-cpu=<MB> Memory required per allocated CPU. sbatch - Submits batch scripts for execution. srun - Obtains job allocation and executes an application.","title":"Batch Jobs"},{"location":"using/queue/#partitions","text":"Partition Max Memory Duration Max CPUs in Queue general 250GB no limit 3004 interactive 250GB 8 hours 3004","title":"Partitions"},{"location":"using/queue/#job-management","text":"","title":"Job Management"},{"location":"using/queue/#squeue","text":"To view your job status in the queue","title":"squeue"},{"location":"using/queue/#scancel","text":"Users can use scancel command to cancel their jobs or job arrays. You may see job states of CA or CG during this processes. scancel JOBID scancel -u $USER","title":"scancel"},{"location":"using/queue/#job-accounting","text":"sacct is the command to view all previously run job information. You can get a list of viewable fields by running the command sacct -e To view a past jobs maximum used memory and duration sacct -j JOBID --format=JobID,JobName,MaxRSS,Elapsed Scontrol can be used to view detailed information about your running job including the job script that was submitted. Please send the output of this command if your currently running job is having issues. ~$ scontrol show jobid -dd 846115 JobId=846115 JobName=sleep.sh UserId=ericmartin(1002) GroupId=ericmartin(1002) Priority=3070 Nice=0 Account=htcfadmin QOS=normal JobState=RUNNING Reason=None Dependency=(null) Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 DerivedExitCode=0:0 RunTime=00:00:09 TimeLimit=UNLIMITED TimeMin=N/A SubmitTime=2016-03-09T09:47:08 EligibleTime=2016-03-09T09:47:08 StartTime=2016-03-09T09:47:09 EndTime=Unknown PreemptTime=None SuspendTime=None SecsPreSuspend=0 Partition=general AllocNode:Sid=n082:21380 ReqNodeList=(null) ExcNodeList=(null) NodeList=n082 BatchHost=n082 NumNodes=1 NumCPUs=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:* Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* Nodes=n082 CPU_IDs=3-4 Mem=2000 MinCPUsNode=1 MinMemoryCPU=1000M MinTmpDiskNode=0 Features=(null) Gres=(null) Reservation=(null) Shared=OK Contiguous=0 Licenses=(null) Network=(null) Command=/scratch/htcfadmin/eric/sleep.sh WorkDir=/scratch/htcfadmin/eric StdErr=/scratch/htcfadmin/eric/slurm-846115.out StdIn=/dev/null StdOut=/scratch/htcfadmin/eric/slurm-846115.out BatchScript= #!/bin/bash #SBATCH -n 2 #SBATCH -N 1 module load bowtie2 sleep 100 More information on usage is available at http://slurm.schedmd.com/sacct.html . More information available here: * http://slurm.schedmd.com/overview.html * http://slurm.schedmd.com/tutorials.html * http://slurm.schedmd.com/faq.html","title":"Job Accounting"}]}